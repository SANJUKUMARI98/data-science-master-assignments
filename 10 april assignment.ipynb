{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16139d43-56ad-4d30-a999-856e97fa825c",
   "metadata": {},
   "source": [
    "# 10 th april assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73836bd1-bb45-4a8b-8cf3-12723a9ad11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7567d8-50a7-4c2d-9bea-c1b50ca8d0f6",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4878c774-b178-4473-8b3e-479b41e4a2cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 4) (4208287461.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    p(h)=probability of event that employees don't have health insaurance\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
     ]
    }
   ],
   "source": [
    "answeer\n",
    "p(H)= probability of event that the employee have health insaurance\n",
    "P(H)=70%\n",
    "p(h)=probability of event that employees don't have health insaurance\n",
    "p(h)=30%\n",
    "p(H/smoker)= 40%\n",
    "p(H/not smoker)=60%\n",
    "p(smoker/H)=?\n",
    "\n",
    "p(H/smoker)=p(H)*p(smoker/H)/p(smoker)\n",
    "\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we need to use Bayes' theorem:\n",
    "\n",
    "P(smoker | uses insurance plan) = P(uses insurance plan | smoker) * P(smoker) / P(uses insurance plan)\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(uses insurance plan) = 0.70 (70% of the employees use the plan)\n",
    "P(smoker | uses insurance plan) = ?\n",
    "P(uses insurance plan | smoker) = 0.40 (40% of the employees who use the plan are smokers)\n",
    "P(smoker) = ?\n",
    "To find P(smoker), we need to use the law of total probability:\n",
    "\n",
    "P(smoker) = P(smoker | uses insurance plan) * P(uses insurance plan) + P(smoker | does not use insurance plan) * P(does not use insurance plan)\n",
    "\n",
    "We are not given P(smoker | does not use insurance plan), but we can assume that it is lower than P(smoker | uses insurance plan) since smokers are more likely to use the insurance plan. Let's assume P(smoker | does not use insurance plan) = 0.20.\n",
    "\n",
    "P(smoker) = P(smoker | uses insurance plan) * 0.70 + 0.20 * 0.30\n",
    "P(smoker) = 0.49 * P(smoker | uses insurance plan)\n",
    "\n",
    "Now we can substitute this into Bayes' theorem:\n",
    "\n",
    "P(smoker | uses insurance plan) = P(uses insurance plan | smoker) * P(smoker) / P(uses insurance plan)\n",
    "P(smoker | uses insurance plan) = 0.40 * 0.49 * P(smoker | uses insurance plan) / 0.70\n",
    "P(smoker | uses insurance plan) = 0.28\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.28 or 28%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc3922-7f37-4469-ac9b-8ef142634701",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6530c043-b70c-47d7-bf28-e5cc40912f3f",
   "metadata": {},
   "source": [
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are algorithms used in machine learning for classification tasks. However, there are some key differences between them:\n",
    "\n",
    "Nature of input data: Bernoulli Naive Bayes is used for binary input data, where each feature can take on only one of two values (usually 0 or 1), while Multinomial Naive Bayes is used for discrete input data, where each feature represents the frequency of a particular word or term.\n",
    "\n",
    "Feature presence vs frequency: In Bernoulli Naive Bayes, the presence or absence of a feature is used to determine the class probability, while in Multinomial Naive Bayes, the frequency of a feature (i.e., the number of times it appears in the input) is used to determine the class probability.\n",
    "\n",
    "Independence assumption: Both algorithms rely on the Naive Bayes assumption of feature independence, which means that the probability of a particular feature occurring is independent of the probability of any other feature occurring. However, Bernoulli Naive Bayes assumes that the features are binary and therefore only cares about their presence or absence, while Multinomial Naive Bayes assumes that the features are discrete counts and therefore cares about their frequency.\n",
    "\n",
    "Application: Bernoulli Naive Bayes is commonly used in document classification tasks, where the presence or absence of certain words (features) is used to determine the class of a document (e.g., spam or non-spam). Multinomial Naive Bayes is commonly used in text classification tasks, where the frequency of certain words (features) is used to determine the class of a document (e.g., sentiment analysis).\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used for binary input data where the presence or absence of a feature is important, while Multinomial Naive Bayes is used for discrete input data where the frequency of a feature is important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fc83e-5ea4-40ea-ba86-2e6c01e45da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb955382-ae97-4753-85a4-d605c798fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `values` not found.\n"
     ]
    }
   ],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bd0f614-e5f3-4798-95aa-8af68e3a8814",
   "metadata": {},
   "source": [
    "In Bernoulli Naive Bayes, missing values can be handled in different ways depending on the implementation. Here are some common strategies:\n",
    "\n",
    "Removing missing values: The simplest approach is to remove any samples or features that contain missing values. However, this approach can result in loss of information and reduced sample size.\n",
    "\n",
    "Imputing missing values: Another approach is to impute (fill in) the missing values with some estimate. This can be done by assigning a default value (e.g., 0 or 1), using the mean or median value of the feature, or using a machine learning algorithm to predict the missing values based on the other features. However, the imputed values may not accurately reflect the true values and can introduce bias into the analysis.\n",
    "\n",
    "Treating missing values as a separate category: In some cases, missing values can be treated as a separate category, with its own probability estimate. This can be useful if missing values represent a meaningful category, such as \"unknown\" or \"not applicable\". However, this approach can also introduce bias if the missing values are not missing at random.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes can handle missing values by removing them, imputing them, or treating them as a separate category, depending on the specifics of the problem and the available data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b972d81-1956-4c33-9cb2-b1360c6a3640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70783fe3-c5d9-4f60-b6d8-feeb1d2e31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `classification` not found.\n"
     ]
    }
   ],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d5225-a57e-4e4f-b641-af7b09c4e780",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. In fact, Gaussian Naive Bayes is one of the most popular algorithms for multi-class classification, along with other Naive Bayes variants such as Multinomial Naive Bayes and Bernoulli Naive Bayes.\n",
    "\n",
    "The basic idea behind using Gaussian Naive Bayes for multi-class classification is to treat each class as a separate binary classification problem, with the goal of determining the probability that a given sample belongs to each class. The algorithm computes the conditional probability of each class given the input features, using a Gaussian probability density function to model the distribution of each feature for each class.\n",
    "\n",
    "To make a prediction for a new sample, the algorithm computes the posterior probability of each class using Bayes' theorem, and selects the class with the highest probability as the predicted class. This process is repeated for each sample in the test set, resulting in a set of predicted classes that can be compared to the true classes to evaluate the performance of the algorithm.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be used for multi-class classification by treating each class as a separate binary classification problem and using a Gaussian probability density function to model the distribution of each feature for each class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead9eff-1d6f-4589-a020-f88e2176e85b",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository\n",
    "link through your dashboard. Make sure the repository is public.\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ab983-ac20-4fa5-8dbe-b5ab4c27ec29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92569235-ebae-4a81-adb5-085a14f8f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('spambase dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2057e3b-cf65-400f-8c5d-d84a7c95ffcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...  0.41  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
       "\n",
       "    0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8247346-4c5f-4176-93e3-b47ec0a72f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4600 entries, 0 to 4599\n",
      "Data columns (total 58 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4600 non-null   float64\n",
      " 1   0.64    4600 non-null   float64\n",
      " 2   0.64.1  4600 non-null   float64\n",
      " 3   0.1     4600 non-null   float64\n",
      " 4   0.32    4600 non-null   float64\n",
      " 5   0.2     4600 non-null   float64\n",
      " 6   0.3     4600 non-null   float64\n",
      " 7   0.4     4600 non-null   float64\n",
      " 8   0.5     4600 non-null   float64\n",
      " 9   0.6     4600 non-null   float64\n",
      " 10  0.7     4600 non-null   float64\n",
      " 11  0.64.2  4600 non-null   float64\n",
      " 12  0.8     4600 non-null   float64\n",
      " 13  0.9     4600 non-null   float64\n",
      " 14  0.10    4600 non-null   float64\n",
      " 15  0.32.1  4600 non-null   float64\n",
      " 16  0.11    4600 non-null   float64\n",
      " 17  1.29    4600 non-null   float64\n",
      " 18  1.93    4600 non-null   float64\n",
      " 19  0.12    4600 non-null   float64\n",
      " 20  0.96    4600 non-null   float64\n",
      " 21  0.13    4600 non-null   float64\n",
      " 22  0.14    4600 non-null   float64\n",
      " 23  0.15    4600 non-null   float64\n",
      " 24  0.16    4600 non-null   float64\n",
      " 25  0.17    4600 non-null   float64\n",
      " 26  0.18    4600 non-null   float64\n",
      " 27  0.19    4600 non-null   float64\n",
      " 28  0.20    4600 non-null   float64\n",
      " 29  0.21    4600 non-null   float64\n",
      " 30  0.22    4600 non-null   float64\n",
      " 31  0.23    4600 non-null   float64\n",
      " 32  0.24    4600 non-null   float64\n",
      " 33  0.25    4600 non-null   float64\n",
      " 34  0.26    4600 non-null   float64\n",
      " 35  0.27    4600 non-null   float64\n",
      " 36  0.28    4600 non-null   float64\n",
      " 37  0.29    4600 non-null   float64\n",
      " 38  0.30    4600 non-null   float64\n",
      " 39  0.31    4600 non-null   float64\n",
      " 40  0.33    4600 non-null   float64\n",
      " 41  0.34    4600 non-null   float64\n",
      " 42  0.35    4600 non-null   float64\n",
      " 43  0.36    4600 non-null   float64\n",
      " 44  0.37    4600 non-null   float64\n",
      " 45  0.38    4600 non-null   float64\n",
      " 46  0.39    4600 non-null   float64\n",
      " 47  0.40    4600 non-null   float64\n",
      " 48  0.41    4600 non-null   float64\n",
      " 49  0.42    4600 non-null   float64\n",
      " 50  0.43    4600 non-null   float64\n",
      " 51  0.778   4600 non-null   float64\n",
      " 52  0.44    4600 non-null   float64\n",
      " 53  0.45    4600 non-null   float64\n",
      " 54  3.756   4600 non-null   float64\n",
      " 55  61      4600 non-null   int64  \n",
      " 56  278     4600 non-null   int64  \n",
      " 57  1       4600 non-null   int64  \n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b945b7c2-b18c-4383-9524-07cd0d3d3630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104576</td>\n",
       "      <td>0.212922</td>\n",
       "      <td>0.280578</td>\n",
       "      <td>0.065439</td>\n",
       "      <td>0.312222</td>\n",
       "      <td>0.095922</td>\n",
       "      <td>0.114233</td>\n",
       "      <td>0.105317</td>\n",
       "      <td>0.090087</td>\n",
       "      <td>0.239465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038583</td>\n",
       "      <td>0.139061</td>\n",
       "      <td>0.016980</td>\n",
       "      <td>0.268960</td>\n",
       "      <td>0.075827</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>5.191827</td>\n",
       "      <td>52.170870</td>\n",
       "      <td>283.290435</td>\n",
       "      <td>0.393913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305387</td>\n",
       "      <td>1.290700</td>\n",
       "      <td>0.504170</td>\n",
       "      <td>1.395303</td>\n",
       "      <td>0.672586</td>\n",
       "      <td>0.273850</td>\n",
       "      <td>0.391480</td>\n",
       "      <td>0.401112</td>\n",
       "      <td>0.278643</td>\n",
       "      <td>0.644816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243497</td>\n",
       "      <td>0.270377</td>\n",
       "      <td>0.109406</td>\n",
       "      <td>0.815726</td>\n",
       "      <td>0.245906</td>\n",
       "      <td>0.429388</td>\n",
       "      <td>31.732891</td>\n",
       "      <td>194.912453</td>\n",
       "      <td>606.413764</td>\n",
       "      <td>0.488669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.275500</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314250</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.705250</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>265.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         0.64       0.64.1          0.1         0.32  \\\n",
       "count  4600.000000  4600.000000  4600.000000  4600.000000  4600.000000   \n",
       "mean      0.104576     0.212922     0.280578     0.065439     0.312222   \n",
       "std       0.305387     1.290700     0.504170     1.395303     0.672586   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.382500   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "               0.2          0.3          0.4          0.5          0.6  ...  \\\n",
       "count  4600.000000  4600.000000  4600.000000  4600.000000  4600.000000  ...   \n",
       "mean      0.095922     0.114233     0.105317     0.090087     0.239465  ...   \n",
       "std       0.273850     0.391480     0.401112     0.278643     0.644816  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000  ...   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000  ...   \n",
       "\n",
       "              0.41         0.42         0.43        0.778         0.44  \\\n",
       "count  4600.000000  4600.000000  4600.000000  4600.000000  4600.000000   \n",
       "mean      0.038583     0.139061     0.016980     0.268960     0.075827   \n",
       "std       0.243497     0.270377     0.109406     0.815726     0.245906   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.314250     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "              0.45        3.756           61           278            1  \n",
       "count  4600.000000  4600.000000  4600.000000   4600.000000  4600.000000  \n",
       "mean      0.044248     5.191827    52.170870    283.290435     0.393913  \n",
       "std       0.429388    31.732891   194.912453    606.413764     0.488669  \n",
       "min       0.000000     1.000000     1.000000      1.000000     0.000000  \n",
       "25%       0.000000     1.588000     6.000000     35.000000     0.000000  \n",
       "50%       0.000000     2.275500    15.000000     95.000000     0.000000  \n",
       "75%       0.000000     3.705250    43.000000    265.250000     1.000000  \n",
       "max      19.829000  1102.500000  9989.000000  15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa37a69a-0651-4ec3-a3bd-e44f4cbce4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016735</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.023120</td>\n",
       "      <td>0.059650</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>0.106241</td>\n",
       "      <td>0.041171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026517</td>\n",
       "      <td>-0.021235</td>\n",
       "      <td>-0.033313</td>\n",
       "      <td>0.058342</td>\n",
       "      <td>0.117398</td>\n",
       "      <td>-0.008852</td>\n",
       "      <td>0.044488</td>\n",
       "      <td>0.061387</td>\n",
       "      <td>0.089165</td>\n",
       "      <td>0.126323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.64</th>\n",
       "      <td>-0.016735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033579</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>-0.023761</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>-0.016261</td>\n",
       "      <td>-0.003803</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007270</td>\n",
       "      <td>-0.049802</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>-0.009584</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-0.022679</td>\n",
       "      <td>-0.030318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.64.1</th>\n",
       "      <td>0.065684</td>\n",
       "      <td>-0.033579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.020240</td>\n",
       "      <td>0.077737</td>\n",
       "      <td>0.087624</td>\n",
       "      <td>0.036725</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.093843</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033190</td>\n",
       "      <td>-0.016417</td>\n",
       "      <td>-0.033098</td>\n",
       "      <td>0.108054</td>\n",
       "      <td>0.087671</td>\n",
       "      <td>-0.003320</td>\n",
       "      <td>0.097410</td>\n",
       "      <td>0.107462</td>\n",
       "      <td>0.070119</td>\n",
       "      <td>0.196840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.013270</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>-0.020240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>0.019781</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>-0.002458</td>\n",
       "      <td>-0.004951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.007150</td>\n",
       "      <td>-0.003132</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.057394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.32</th>\n",
       "      <td>0.023120</td>\n",
       "      <td>-0.023761</td>\n",
       "      <td>0.077737</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054055</td>\n",
       "      <td>0.147338</td>\n",
       "      <td>0.029599</td>\n",
       "      <td>0.020824</td>\n",
       "      <td>0.034497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032759</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.241958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.059650</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>0.087624</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>0.054055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061142</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.117416</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019131</td>\n",
       "      <td>-0.008745</td>\n",
       "      <td>-0.015145</td>\n",
       "      <td>0.065094</td>\n",
       "      <td>0.105671</td>\n",
       "      <td>0.019887</td>\n",
       "      <td>-0.010282</td>\n",
       "      <td>0.090177</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.232741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.007647</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.036725</td>\n",
       "      <td>0.019781</td>\n",
       "      <td>0.147338</td>\n",
       "      <td>0.061142</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044530</td>\n",
       "      <td>0.050767</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033099</td>\n",
       "      <td>-0.051919</td>\n",
       "      <td>-0.027663</td>\n",
       "      <td>0.053748</td>\n",
       "      <td>0.070109</td>\n",
       "      <td>0.046606</td>\n",
       "      <td>0.041563</td>\n",
       "      <td>0.059680</td>\n",
       "      <td>-0.008344</td>\n",
       "      <td>0.332255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>-0.003970</td>\n",
       "      <td>-0.016261</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.029599</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.044530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>0.083110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027441</td>\n",
       "      <td>-0.032525</td>\n",
       "      <td>-0.019557</td>\n",
       "      <td>0.031491</td>\n",
       "      <td>0.057894</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>0.206915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.106241</td>\n",
       "      <td>-0.003803</td>\n",
       "      <td>0.093843</td>\n",
       "      <td>-0.002458</td>\n",
       "      <td>0.020824</td>\n",
       "      <td>0.117416</td>\n",
       "      <td>0.050767</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014657</td>\n",
       "      <td>-0.031040</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>0.149347</td>\n",
       "      <td>-0.000530</td>\n",
       "      <td>0.111306</td>\n",
       "      <td>0.189252</td>\n",
       "      <td>0.248726</td>\n",
       "      <td>0.231680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.041171</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>-0.004951</td>\n",
       "      <td>0.034497</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.083110</td>\n",
       "      <td>0.130601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>0.044822</td>\n",
       "      <td>0.073674</td>\n",
       "      <td>0.103314</td>\n",
       "      <td>0.087274</td>\n",
       "      <td>0.139088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.188441</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>0.048304</td>\n",
       "      <td>-0.012980</td>\n",
       "      <td>0.068383</td>\n",
       "      <td>0.053878</td>\n",
       "      <td>0.159562</td>\n",
       "      <td>0.128481</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.125298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032420</td>\n",
       "      <td>-0.055124</td>\n",
       "      <td>-0.025193</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>0.070208</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.086795</td>\n",
       "      <td>0.115056</td>\n",
       "      <td>0.234651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.64.2</th>\n",
       "      <td>0.105811</td>\n",
       "      <td>-0.040406</td>\n",
       "      <td>0.083197</td>\n",
       "      <td>-0.019220</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>-0.002967</td>\n",
       "      <td>0.030352</td>\n",
       "      <td>0.071167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027707</td>\n",
       "      <td>-0.030928</td>\n",
       "      <td>-0.044962</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.016730</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>-0.010001</td>\n",
       "      <td>0.021773</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>0.007711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.066416</td>\n",
       "      <td>-0.018836</td>\n",
       "      <td>0.047644</td>\n",
       "      <td>-0.013203</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>0.077609</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.034717</td>\n",
       "      <td>0.045713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023456</td>\n",
       "      <td>-0.051188</td>\n",
       "      <td>-0.028294</td>\n",
       "      <td>0.040781</td>\n",
       "      <td>0.205889</td>\n",
       "      <td>-0.014202</td>\n",
       "      <td>-0.013450</td>\n",
       "      <td>0.041965</td>\n",
       "      <td>0.105150</td>\n",
       "      <td>0.133034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.036768</td>\n",
       "      <td>-0.009194</td>\n",
       "      <td>0.008580</td>\n",
       "      <td>0.012006</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.009660</td>\n",
       "      <td>-0.022734</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019051</td>\n",
       "      <td>-0.005824</td>\n",
       "      <td>-0.014355</td>\n",
       "      <td>-0.008476</td>\n",
       "      <td>0.080942</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.060994</td>\n",
       "      <td>0.169258</td>\n",
       "      <td>0.060085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.028425</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>0.122150</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.056177</td>\n",
       "      <td>0.173055</td>\n",
       "      <td>0.042892</td>\n",
       "      <td>0.072772</td>\n",
       "      <td>0.238426</td>\n",
       "      <td>0.160531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018283</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>-0.003118</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.123843</td>\n",
       "      <td>-0.005450</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.213995</td>\n",
       "      <td>0.151626</td>\n",
       "      <td>0.195987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.32.1</th>\n",
       "      <td>0.059393</td>\n",
       "      <td>-0.009123</td>\n",
       "      <td>0.063896</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>0.083024</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.128442</td>\n",
       "      <td>0.051121</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.025609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026838</td>\n",
       "      <td>-0.046570</td>\n",
       "      <td>-0.029557</td>\n",
       "      <td>0.104253</td>\n",
       "      <td>0.049959</td>\n",
       "      <td>0.035536</td>\n",
       "      <td>0.015037</td>\n",
       "      <td>0.026527</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.263236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>0.081906</td>\n",
       "      <td>-0.018348</td>\n",
       "      <td>0.036314</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.143446</td>\n",
       "      <td>0.064114</td>\n",
       "      <td>0.187965</td>\n",
       "      <td>0.216408</td>\n",
       "      <td>0.158371</td>\n",
       "      <td>0.081339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031554</td>\n",
       "      <td>-0.035935</td>\n",
       "      <td>-0.036702</td>\n",
       "      <td>0.077097</td>\n",
       "      <td>0.098304</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.062676</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.263338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.29</th>\n",
       "      <td>0.053504</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>0.121665</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.062368</td>\n",
       "      <td>0.078547</td>\n",
       "      <td>0.122202</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>0.098998</td>\n",
       "      <td>0.036163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039466</td>\n",
       "      <td>-0.035682</td>\n",
       "      <td>-0.017377</td>\n",
       "      <td>0.039088</td>\n",
       "      <td>0.064042</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>-0.007963</td>\n",
       "      <td>0.075136</td>\n",
       "      <td>0.046390</td>\n",
       "      <td>0.203777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.93</th>\n",
       "      <td>0.128256</td>\n",
       "      <td>-0.055488</td>\n",
       "      <td>0.139314</td>\n",
       "      <td>-0.010832</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.095518</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.039028</td>\n",
       "      <td>0.093522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044309</td>\n",
       "      <td>-0.128869</td>\n",
       "      <td>-0.063822</td>\n",
       "      <td>0.153368</td>\n",
       "      <td>0.091481</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>-0.030591</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>-0.007307</td>\n",
       "      <td>0.273657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.12</th>\n",
       "      <td>0.021282</td>\n",
       "      <td>-0.015794</td>\n",
       "      <td>0.031139</td>\n",
       "      <td>-0.005383</td>\n",
       "      <td>0.031527</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>0.046124</td>\n",
       "      <td>0.109155</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>0.030846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020856</td>\n",
       "      <td>-0.021450</td>\n",
       "      <td>-0.012077</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.034937</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.067138</td>\n",
       "      <td>0.099465</td>\n",
       "      <td>0.075751</td>\n",
       "      <td>0.189839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.197061</td>\n",
       "      <td>-0.018200</td>\n",
       "      <td>0.156641</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.136605</td>\n",
       "      <td>0.106844</td>\n",
       "      <td>0.130804</td>\n",
       "      <td>0.156914</td>\n",
       "      <td>0.159123</td>\n",
       "      <td>0.098084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058656</td>\n",
       "      <td>-0.085170</td>\n",
       "      <td>-0.045465</td>\n",
       "      <td>0.084004</td>\n",
       "      <td>0.141659</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>0.041067</td>\n",
       "      <td>0.085320</td>\n",
       "      <td>0.051797</td>\n",
       "      <td>0.383265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.13</th>\n",
       "      <td>-0.024358</td>\n",
       "      <td>-0.008841</td>\n",
       "      <td>-0.035665</td>\n",
       "      <td>0.028101</td>\n",
       "      <td>-0.020207</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>-0.016199</td>\n",
       "      <td>-0.019656</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416606</td>\n",
       "      <td>-0.046259</td>\n",
       "      <td>-0.001141</td>\n",
       "      <td>-0.004823</td>\n",
       "      <td>-0.011044</td>\n",
       "      <td>0.184425</td>\n",
       "      <td>0.021496</td>\n",
       "      <td>0.027776</td>\n",
       "      <td>0.103954</td>\n",
       "      <td>0.091907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.14</th>\n",
       "      <td>0.134053</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>0.011365</td>\n",
       "      <td>0.070039</td>\n",
       "      <td>0.211437</td>\n",
       "      <td>0.064778</td>\n",
       "      <td>0.089211</td>\n",
       "      <td>0.126782</td>\n",
       "      <td>0.096788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027372</td>\n",
       "      <td>-0.033208</td>\n",
       "      <td>-0.000477</td>\n",
       "      <td>0.070146</td>\n",
       "      <td>0.310957</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>0.123040</td>\n",
       "      <td>0.165978</td>\n",
       "      <td>0.334924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>0.188143</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.041181</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.059314</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>0.034115</td>\n",
       "      <td>0.099448</td>\n",
       "      <td>0.052113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019147</td>\n",
       "      <td>-0.033138</td>\n",
       "      <td>-0.020806</td>\n",
       "      <td>0.051107</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.080993</td>\n",
       "      <td>0.216206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>-0.072530</td>\n",
       "      <td>-0.043461</td>\n",
       "      <td>-0.087879</td>\n",
       "      <td>-0.015185</td>\n",
       "      <td>-0.072502</td>\n",
       "      <td>-0.084430</td>\n",
       "      <td>-0.089517</td>\n",
       "      <td>-0.053058</td>\n",
       "      <td>-0.069955</td>\n",
       "      <td>-0.033561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.136948</td>\n",
       "      <td>0.039712</td>\n",
       "      <td>-0.090822</td>\n",
       "      <td>-0.086658</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>-0.017289</td>\n",
       "      <td>-0.051204</td>\n",
       "      <td>-0.043269</td>\n",
       "      <td>-0.256680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.17</th>\n",
       "      <td>-0.061710</td>\n",
       "      <td>-0.038191</td>\n",
       "      <td>-0.062417</td>\n",
       "      <td>-0.013711</td>\n",
       "      <td>-0.075456</td>\n",
       "      <td>-0.087296</td>\n",
       "      <td>-0.080350</td>\n",
       "      <td>-0.041468</td>\n",
       "      <td>-0.049797</td>\n",
       "      <td>-0.013070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>0.144743</td>\n",
       "      <td>0.064340</td>\n",
       "      <td>-0.078331</td>\n",
       "      <td>-0.081219</td>\n",
       "      <td>-0.020698</td>\n",
       "      <td>-0.024238</td>\n",
       "      <td>-0.051804</td>\n",
       "      <td>-0.059603</td>\n",
       "      <td>-0.232928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.18</th>\n",
       "      <td>-0.066443</td>\n",
       "      <td>-0.030291</td>\n",
       "      <td>-0.108857</td>\n",
       "      <td>-0.010687</td>\n",
       "      <td>-0.088011</td>\n",
       "      <td>-0.069070</td>\n",
       "      <td>-0.065908</td>\n",
       "      <td>-0.057202</td>\n",
       "      <td>-0.064625</td>\n",
       "      <td>-0.067837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022732</td>\n",
       "      <td>-0.028774</td>\n",
       "      <td>-0.017684</td>\n",
       "      <td>-0.067472</td>\n",
       "      <td>-0.068744</td>\n",
       "      <td>-0.020566</td>\n",
       "      <td>-0.025506</td>\n",
       "      <td>-0.054398</td>\n",
       "      <td>-0.096549</td>\n",
       "      <td>-0.183374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.19</th>\n",
       "      <td>-0.048698</td>\n",
       "      <td>-0.029205</td>\n",
       "      <td>-0.050615</td>\n",
       "      <td>-0.010370</td>\n",
       "      <td>-0.061500</td>\n",
       "      <td>-0.066242</td>\n",
       "      <td>-0.066962</td>\n",
       "      <td>-0.050002</td>\n",
       "      <td>-0.056781</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025028</td>\n",
       "      <td>0.313820</td>\n",
       "      <td>0.031971</td>\n",
       "      <td>-0.063467</td>\n",
       "      <td>-0.061457</td>\n",
       "      <td>-0.011443</td>\n",
       "      <td>-0.013760</td>\n",
       "      <td>-0.038770</td>\n",
       "      <td>-0.067597</td>\n",
       "      <td>-0.158765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>-0.041264</td>\n",
       "      <td>-0.021928</td>\n",
       "      <td>-0.057703</td>\n",
       "      <td>-0.007799</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>-0.048686</td>\n",
       "      <td>-0.048493</td>\n",
       "      <td>-0.037057</td>\n",
       "      <td>-0.044852</td>\n",
       "      <td>-0.026917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018508</td>\n",
       "      <td>0.158580</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>-0.042309</td>\n",
       "      <td>-0.050243</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.014938</td>\n",
       "      <td>-0.034731</td>\n",
       "      <td>-0.056629</td>\n",
       "      <td>-0.133501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>-0.052817</td>\n",
       "      <td>-0.027492</td>\n",
       "      <td>-0.032514</td>\n",
       "      <td>-0.010478</td>\n",
       "      <td>-0.052066</td>\n",
       "      <td>-0.048145</td>\n",
       "      <td>-0.058116</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019853</td>\n",
       "      <td>0.224174</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>-0.061666</td>\n",
       "      <td>-0.065491</td>\n",
       "      <td>0.082589</td>\n",
       "      <td>-0.016601</td>\n",
       "      <td>-0.038999</td>\n",
       "      <td>-0.064116</td>\n",
       "      <td>-0.171063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>-0.039079</td>\n",
       "      <td>-0.018086</td>\n",
       "      <td>-0.038905</td>\n",
       "      <td>-0.007531</td>\n",
       "      <td>-0.042535</td>\n",
       "      <td>-0.046396</td>\n",
       "      <td>-0.046290</td>\n",
       "      <td>-0.035825</td>\n",
       "      <td>-0.040170</td>\n",
       "      <td>-0.024436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016285</td>\n",
       "      <td>0.233381</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>-0.045253</td>\n",
       "      <td>-0.047486</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>-0.010899</td>\n",
       "      <td>-0.027448</td>\n",
       "      <td>-0.045923</td>\n",
       "      <td>-0.126890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.23</th>\n",
       "      <td>-0.032069</td>\n",
       "      <td>-0.003316</td>\n",
       "      <td>-0.061852</td>\n",
       "      <td>-0.006718</td>\n",
       "      <td>-0.026748</td>\n",
       "      <td>-0.036847</td>\n",
       "      <td>-0.040548</td>\n",
       "      <td>-0.034285</td>\n",
       "      <td>-0.033994</td>\n",
       "      <td>-0.015149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008858</td>\n",
       "      <td>0.304672</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>-0.041511</td>\n",
       "      <td>-0.043494</td>\n",
       "      <td>-0.010738</td>\n",
       "      <td>-0.010500</td>\n",
       "      <td>-0.027730</td>\n",
       "      <td>-0.046797</td>\n",
       "      <td>-0.114195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.24</th>\n",
       "      <td>-0.041028</td>\n",
       "      <td>-0.024891</td>\n",
       "      <td>-0.054735</td>\n",
       "      <td>-0.008077</td>\n",
       "      <td>-0.031997</td>\n",
       "      <td>-0.034178</td>\n",
       "      <td>-0.041384</td>\n",
       "      <td>-0.039230</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.035381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>0.028637</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>-0.048472</td>\n",
       "      <td>-0.048113</td>\n",
       "      <td>-0.009932</td>\n",
       "      <td>-0.015511</td>\n",
       "      <td>-0.025917</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>-0.119904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>-0.027701</td>\n",
       "      <td>-0.004292</td>\n",
       "      <td>-0.061687</td>\n",
       "      <td>-0.006730</td>\n",
       "      <td>-0.026960</td>\n",
       "      <td>-0.037327</td>\n",
       "      <td>-0.040920</td>\n",
       "      <td>-0.034820</td>\n",
       "      <td>-0.033612</td>\n",
       "      <td>-0.014446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.303599</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>-0.038608</td>\n",
       "      <td>-0.039855</td>\n",
       "      <td>-0.010638</td>\n",
       "      <td>-0.002406</td>\n",
       "      <td>-0.024531</td>\n",
       "      <td>-0.044529</td>\n",
       "      <td>-0.112734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>-0.044969</td>\n",
       "      <td>-0.024044</td>\n",
       "      <td>-0.048307</td>\n",
       "      <td>-0.006124</td>\n",
       "      <td>-0.049731</td>\n",
       "      <td>-0.054331</td>\n",
       "      <td>-0.053215</td>\n",
       "      <td>-0.035186</td>\n",
       "      <td>-0.041862</td>\n",
       "      <td>-0.020108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021599</td>\n",
       "      <td>0.200697</td>\n",
       "      <td>0.034428</td>\n",
       "      <td>-0.048797</td>\n",
       "      <td>-0.048961</td>\n",
       "      <td>-0.009655</td>\n",
       "      <td>-0.013709</td>\n",
       "      <td>-0.030234</td>\n",
       "      <td>-0.045964</td>\n",
       "      <td>-0.149197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.27</th>\n",
       "      <td>-0.054692</td>\n",
       "      <td>-0.028181</td>\n",
       "      <td>-0.046469</td>\n",
       "      <td>-0.006517</td>\n",
       "      <td>-0.048844</td>\n",
       "      <td>-0.052838</td>\n",
       "      <td>-0.053995</td>\n",
       "      <td>-0.033761</td>\n",
       "      <td>-0.056288</td>\n",
       "      <td>-0.016975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018955</td>\n",
       "      <td>0.245435</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>-0.060349</td>\n",
       "      <td>-0.057951</td>\n",
       "      <td>0.006447</td>\n",
       "      <td>-0.019188</td>\n",
       "      <td>-0.038098</td>\n",
       "      <td>-0.045792</td>\n",
       "      <td>-0.136093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.28</th>\n",
       "      <td>-0.057338</td>\n",
       "      <td>-0.023990</td>\n",
       "      <td>-0.066970</td>\n",
       "      <td>-0.007765</td>\n",
       "      <td>-0.072599</td>\n",
       "      <td>-0.057491</td>\n",
       "      <td>-0.052057</td>\n",
       "      <td>-0.017484</td>\n",
       "      <td>-0.033267</td>\n",
       "      <td>-0.004970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052127</td>\n",
       "      <td>0.107642</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>-0.054537</td>\n",
       "      <td>-0.063918</td>\n",
       "      <td>-0.022645</td>\n",
       "      <td>-0.014427</td>\n",
       "      <td>-0.033201</td>\n",
       "      <td>-0.003490</td>\n",
       "      <td>-0.177990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.29</th>\n",
       "      <td>-0.007965</td>\n",
       "      <td>-0.008918</td>\n",
       "      <td>0.032418</td>\n",
       "      <td>-0.002669</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>-0.017923</td>\n",
       "      <td>-0.014785</td>\n",
       "      <td>-0.012123</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>-0.017955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007884</td>\n",
       "      <td>-0.010437</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.012913</td>\n",
       "      <td>-0.003629</td>\n",
       "      <td>-0.006012</td>\n",
       "      <td>-0.009486</td>\n",
       "      <td>-0.013897</td>\n",
       "      <td>-0.031024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>-0.011148</td>\n",
       "      <td>-0.019111</td>\n",
       "      <td>-0.014782</td>\n",
       "      <td>-0.004604</td>\n",
       "      <td>-0.042044</td>\n",
       "      <td>-0.047633</td>\n",
       "      <td>-0.046990</td>\n",
       "      <td>-0.030403</td>\n",
       "      <td>-0.040857</td>\n",
       "      <td>-0.016106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034486</td>\n",
       "      <td>0.107912</td>\n",
       "      <td>0.038469</td>\n",
       "      <td>-0.024822</td>\n",
       "      <td>-0.044526</td>\n",
       "      <td>-0.011330</td>\n",
       "      <td>-0.014033</td>\n",
       "      <td>-0.029227</td>\n",
       "      <td>-0.049257</td>\n",
       "      <td>-0.122803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>-0.036110</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.047040</td>\n",
       "      <td>-0.007645</td>\n",
       "      <td>-0.021442</td>\n",
       "      <td>-0.029880</td>\n",
       "      <td>-0.022133</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>-0.009880</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>0.268689</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>-0.032486</td>\n",
       "      <td>-0.016737</td>\n",
       "      <td>-0.010666</td>\n",
       "      <td>-0.003947</td>\n",
       "      <td>-0.004833</td>\n",
       "      <td>-0.028806</td>\n",
       "      <td>-0.064762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.33</th>\n",
       "      <td>-0.009712</td>\n",
       "      <td>-0.015411</td>\n",
       "      <td>-0.030939</td>\n",
       "      <td>-0.005672</td>\n",
       "      <td>-0.047505</td>\n",
       "      <td>-0.029466</td>\n",
       "      <td>-0.033128</td>\n",
       "      <td>-0.003891</td>\n",
       "      <td>-0.035185</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.034404</td>\n",
       "      <td>-0.025895</td>\n",
       "      <td>-0.036618</td>\n",
       "      <td>-0.011757</td>\n",
       "      <td>-0.008896</td>\n",
       "      <td>-0.023657</td>\n",
       "      <td>-0.026373</td>\n",
       "      <td>-0.097359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.34</th>\n",
       "      <td>-0.026083</td>\n",
       "      <td>-0.025165</td>\n",
       "      <td>-0.005784</td>\n",
       "      <td>-0.008097</td>\n",
       "      <td>0.115042</td>\n",
       "      <td>-0.054826</td>\n",
       "      <td>-0.049675</td>\n",
       "      <td>-0.043636</td>\n",
       "      <td>-0.048236</td>\n",
       "      <td>-0.054482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007823</td>\n",
       "      <td>-0.013102</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>-0.038072</td>\n",
       "      <td>-0.043666</td>\n",
       "      <td>-0.003877</td>\n",
       "      <td>-0.017900</td>\n",
       "      <td>-0.034583</td>\n",
       "      <td>-0.056512</td>\n",
       "      <td>-0.136592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>-0.024308</td>\n",
       "      <td>-0.002355</td>\n",
       "      <td>-0.044296</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>-0.048878</td>\n",
       "      <td>-0.030632</td>\n",
       "      <td>-0.049093</td>\n",
       "      <td>-0.004554</td>\n",
       "      <td>-0.034205</td>\n",
       "      <td>0.023184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015378</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.115542</td>\n",
       "      <td>-0.049336</td>\n",
       "      <td>-0.054713</td>\n",
       "      <td>-0.013930</td>\n",
       "      <td>-0.017683</td>\n",
       "      <td>-0.017277</td>\n",
       "      <td>-0.036529</td>\n",
       "      <td>-0.135632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>-0.022126</td>\n",
       "      <td>-0.019730</td>\n",
       "      <td>-0.053448</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>-0.028837</td>\n",
       "      <td>-0.034469</td>\n",
       "      <td>-0.030142</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>-0.026665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007261</td>\n",
       "      <td>-0.003217</td>\n",
       "      <td>-0.010738</td>\n",
       "      <td>-0.033822</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>-0.013158</td>\n",
       "      <td>-0.025917</td>\n",
       "      <td>-0.040661</td>\n",
       "      <td>-0.094576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.37</th>\n",
       "      <td>-0.037128</td>\n",
       "      <td>-0.016397</td>\n",
       "      <td>-0.050621</td>\n",
       "      <td>-0.012960</td>\n",
       "      <td>-0.042336</td>\n",
       "      <td>-0.053661</td>\n",
       "      <td>-0.050831</td>\n",
       "      <td>-0.002440</td>\n",
       "      <td>-0.075581</td>\n",
       "      <td>-0.032089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024709</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.067613</td>\n",
       "      <td>-0.049388</td>\n",
       "      <td>-0.023885</td>\n",
       "      <td>-0.026982</td>\n",
       "      <td>-0.051855</td>\n",
       "      <td>-0.095445</td>\n",
       "      <td>-0.140353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.38</th>\n",
       "      <td>-0.034071</td>\n",
       "      <td>-0.023844</td>\n",
       "      <td>-0.056628</td>\n",
       "      <td>-0.009183</td>\n",
       "      <td>-0.077985</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.056179</td>\n",
       "      <td>-0.037927</td>\n",
       "      <td>-0.056832</td>\n",
       "      <td>-0.030343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015376</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>-0.003174</td>\n",
       "      <td>-0.028820</td>\n",
       "      <td>-0.050123</td>\n",
       "      <td>-0.015044</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>-0.033363</td>\n",
       "      <td>-0.046372</td>\n",
       "      <td>-0.146110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.39</th>\n",
       "      <td>-0.000958</td>\n",
       "      <td>-0.009813</td>\n",
       "      <td>0.029351</td>\n",
       "      <td>-0.003349</td>\n",
       "      <td>-0.026900</td>\n",
       "      <td>-0.014349</td>\n",
       "      <td>-0.017517</td>\n",
       "      <td>-0.006401</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>-0.015552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>-0.003093</td>\n",
       "      <td>-0.004594</td>\n",
       "      <td>-0.017670</td>\n",
       "      <td>-0.018554</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>-0.006465</td>\n",
       "      <td>-0.010153</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>-0.044667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>-0.017763</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.001925</td>\n",
       "      <td>-0.032005</td>\n",
       "      <td>-0.031702</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>-0.021230</td>\n",
       "      <td>-0.026025</td>\n",
       "      <td>-0.016851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002294</td>\n",
       "      <td>-0.012808</td>\n",
       "      <td>-0.006313</td>\n",
       "      <td>-0.026562</td>\n",
       "      <td>-0.030758</td>\n",
       "      <td>-0.008577</td>\n",
       "      <td>-0.008115</td>\n",
       "      <td>-0.016893</td>\n",
       "      <td>-0.010033</td>\n",
       "      <td>-0.084004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>-0.026517</td>\n",
       "      <td>-0.007270</td>\n",
       "      <td>-0.033190</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.032759</td>\n",
       "      <td>-0.019131</td>\n",
       "      <td>-0.033099</td>\n",
       "      <td>-0.027441</td>\n",
       "      <td>-0.014657</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049108</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>0.020562</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.040831</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>-0.059597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.42</th>\n",
       "      <td>-0.021235</td>\n",
       "      <td>-0.049802</td>\n",
       "      <td>-0.016417</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>-0.008745</td>\n",
       "      <td>-0.051919</td>\n",
       "      <td>-0.032525</td>\n",
       "      <td>-0.031040</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>-0.030286</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.023311</td>\n",
       "      <td>0.034361</td>\n",
       "      <td>0.370978</td>\n",
       "      <td>0.112212</td>\n",
       "      <td>-0.089551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.43</th>\n",
       "      <td>-0.033313</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>-0.033098</td>\n",
       "      <td>-0.007150</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.015145</td>\n",
       "      <td>-0.027663</td>\n",
       "      <td>-0.019557</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031749</td>\n",
       "      <td>-0.026411</td>\n",
       "      <td>-0.006866</td>\n",
       "      <td>-0.008181</td>\n",
       "      <td>-0.013992</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>-0.064678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.778</th>\n",
       "      <td>0.058342</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>0.108054</td>\n",
       "      <td>-0.003132</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.065094</td>\n",
       "      <td>0.053748</td>\n",
       "      <td>0.031491</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020562</td>\n",
       "      <td>-0.030286</td>\n",
       "      <td>-0.031749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142962</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>0.054317</td>\n",
       "      <td>0.077389</td>\n",
       "      <td>0.036324</td>\n",
       "      <td>0.241771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.44</th>\n",
       "      <td>0.117398</td>\n",
       "      <td>-0.009584</td>\n",
       "      <td>0.087671</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>0.105671</td>\n",
       "      <td>0.070109</td>\n",
       "      <td>0.057894</td>\n",
       "      <td>0.149347</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.026411</td>\n",
       "      <td>0.142962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012606</td>\n",
       "      <td>0.079995</td>\n",
       "      <td>0.183149</td>\n",
       "      <td>0.201949</td>\n",
       "      <td>0.323769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>-0.008852</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-0.003320</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.019887</td>\n",
       "      <td>0.046606</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>-0.000530</td>\n",
       "      <td>0.044822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.023311</td>\n",
       "      <td>-0.006866</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>0.012606</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>0.042568</td>\n",
       "      <td>0.065105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.756</th>\n",
       "      <td>0.044488</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.097410</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>-0.010282</td>\n",
       "      <td>0.041563</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>0.111306</td>\n",
       "      <td>0.073674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.034361</td>\n",
       "      <td>-0.008181</td>\n",
       "      <td>0.054317</td>\n",
       "      <td>0.079995</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.492639</td>\n",
       "      <td>0.162314</td>\n",
       "      <td>0.110030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.061387</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.107462</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.090177</td>\n",
       "      <td>0.059680</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.189252</td>\n",
       "      <td>0.103314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040831</td>\n",
       "      <td>0.370978</td>\n",
       "      <td>-0.013992</td>\n",
       "      <td>0.077389</td>\n",
       "      <td>0.183149</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>0.492639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475486</td>\n",
       "      <td>0.216121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.089165</td>\n",
       "      <td>-0.022679</td>\n",
       "      <td>0.070119</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>-0.008344</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>0.248726</td>\n",
       "      <td>0.087274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.112212</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.036324</td>\n",
       "      <td>0.201949</td>\n",
       "      <td>0.042568</td>\n",
       "      <td>0.162314</td>\n",
       "      <td>0.475486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126323</td>\n",
       "      <td>-0.030318</td>\n",
       "      <td>0.196840</td>\n",
       "      <td>0.057394</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.232741</td>\n",
       "      <td>0.332255</td>\n",
       "      <td>0.206915</td>\n",
       "      <td>0.231680</td>\n",
       "      <td>0.139088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059597</td>\n",
       "      <td>-0.089551</td>\n",
       "      <td>-0.064678</td>\n",
       "      <td>0.241771</td>\n",
       "      <td>0.323769</td>\n",
       "      <td>0.065105</td>\n",
       "      <td>0.110030</td>\n",
       "      <td>0.216121</td>\n",
       "      <td>0.249208</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      0.64    0.64.1       0.1      0.32       0.2       0.3  \\\n",
       "0       1.000000 -0.016735  0.065684  0.013270  0.023120  0.059650  0.007647   \n",
       "0.64   -0.016735  1.000000 -0.033579 -0.006920 -0.023761 -0.024815  0.003939   \n",
       "0.64.1  0.065684 -0.033579  1.000000 -0.020240  0.077737  0.087624  0.036725   \n",
       "0.1     0.013270 -0.006920 -0.020240  1.000000  0.003238 -0.010018  0.019781   \n",
       "0.32    0.023120 -0.023761  0.077737  0.003238  1.000000  0.054055  0.147338   \n",
       "0.2     0.059650 -0.024815  0.087624 -0.010018  0.054055  1.000000  0.061142   \n",
       "0.3     0.007647  0.003939  0.036725  0.019781  0.147338  0.061142  1.000000   \n",
       "0.4    -0.003970 -0.016261  0.012044  0.010265  0.029599  0.079543  0.044530   \n",
       "0.5     0.106241 -0.003803  0.093843 -0.002458  0.020824  0.117416  0.050767   \n",
       "0.6     0.041171  0.032989  0.032135 -0.004951  0.034497  0.013869  0.056787   \n",
       "0.7     0.188441 -0.006843  0.048304 -0.012980  0.068383  0.053878  0.159562   \n",
       "0.64.2  0.105811 -0.040406  0.083197 -0.019220  0.066788  0.009272 -0.001454   \n",
       "0.8     0.066416 -0.018836  0.047644 -0.013203  0.031127  0.077609  0.013276   \n",
       "0.9     0.036768 -0.009194  0.008580  0.012006  0.003446  0.009660 -0.022734   \n",
       "0.10    0.028425  0.005344  0.122150  0.002705  0.056177  0.173055  0.042892   \n",
       "0.32.1  0.059393 -0.009123  0.063896  0.007433  0.083024  0.019872  0.128442   \n",
       "0.11    0.081906 -0.018348  0.036314  0.003467  0.143446  0.064114  0.187965   \n",
       "1.29    0.053504  0.033366  0.121665  0.019422  0.062368  0.078547  0.122202   \n",
       "1.93    0.128256 -0.055488  0.139314 -0.010832  0.098510  0.095518  0.111803   \n",
       "0.12    0.021282 -0.015794  0.031139 -0.005383  0.031527  0.058967  0.046124   \n",
       "0.96    0.197061 -0.018200  0.156641  0.008177  0.136605  0.106844  0.130804   \n",
       "0.13   -0.024358 -0.008841 -0.035665  0.028101 -0.020207  0.007948 -0.002101   \n",
       "0.14    0.134053 -0.020481  0.123724  0.011365  0.070039  0.211437  0.064778   \n",
       "0.15    0.188143  0.001999  0.041181  0.035358  0.000040  0.059314  0.030562   \n",
       "0.16   -0.072530 -0.043461 -0.087879 -0.015185 -0.072502 -0.084430 -0.089517   \n",
       "0.17   -0.061710 -0.038191 -0.062417 -0.013711 -0.075456 -0.087296 -0.080350   \n",
       "0.18   -0.066443 -0.030291 -0.108857 -0.010687 -0.088011 -0.069070 -0.065908   \n",
       "0.19   -0.048698 -0.029205 -0.050615 -0.010370 -0.061500 -0.066242 -0.066962   \n",
       "0.20   -0.041264 -0.021928 -0.057703 -0.007799  0.032048 -0.048686 -0.048493   \n",
       "0.21   -0.052817 -0.027492 -0.032514 -0.010478 -0.052066 -0.048145 -0.058116   \n",
       "0.22   -0.039079 -0.018086 -0.038905 -0.007531 -0.042535 -0.046396 -0.046290   \n",
       "0.23   -0.032069 -0.003316 -0.061852 -0.006718 -0.026748 -0.036847 -0.040548   \n",
       "0.24   -0.041028 -0.024891 -0.054735 -0.008077 -0.031997 -0.034178 -0.041384   \n",
       "0.25   -0.027701 -0.004292 -0.061687 -0.006730 -0.026960 -0.037327 -0.040920   \n",
       "0.26   -0.044969 -0.024044 -0.048307 -0.006124 -0.049731 -0.054331 -0.053215   \n",
       "0.27   -0.054692 -0.028181 -0.046469 -0.006517 -0.048844 -0.052838 -0.053995   \n",
       "0.28   -0.057338 -0.023990 -0.066970 -0.007765 -0.072599 -0.057491 -0.052057   \n",
       "0.29   -0.007965 -0.008918  0.032418 -0.002669  0.130812 -0.017923 -0.014785   \n",
       "0.30   -0.011148 -0.019111 -0.014782 -0.004604 -0.042044 -0.047633 -0.046990   \n",
       "0.31   -0.036110 -0.014808 -0.047040 -0.007645 -0.021442 -0.029880 -0.022133   \n",
       "0.33   -0.009712 -0.015411 -0.030939 -0.005672 -0.047505 -0.029466 -0.033128   \n",
       "0.34   -0.026083 -0.025165 -0.005784 -0.008097  0.115042 -0.054826 -0.049675   \n",
       "0.35   -0.024308 -0.002355 -0.044296 -0.009270 -0.048878 -0.030632 -0.049093   \n",
       "0.36   -0.022126 -0.019730 -0.053448 -0.005935  0.015234 -0.028837 -0.034469   \n",
       "0.37   -0.037128 -0.016397 -0.050621 -0.012960 -0.042336 -0.053661 -0.050831   \n",
       "0.38   -0.034071 -0.023844 -0.056628 -0.009183 -0.077985 -0.033061 -0.056179   \n",
       "0.39   -0.000958 -0.009813  0.029351 -0.003349 -0.026900 -0.014349 -0.017517   \n",
       "0.40   -0.017763 -0.015739 -0.026328 -0.001925 -0.032005 -0.031702 -0.031415   \n",
       "0.41   -0.026517 -0.007270 -0.033190 -0.000592 -0.032759 -0.019131 -0.033099   \n",
       "0.42   -0.021235 -0.049802 -0.016417 -0.012376 -0.046361 -0.008745 -0.051919   \n",
       "0.43   -0.033313 -0.018516 -0.033098 -0.007150 -0.026389 -0.015145 -0.027663   \n",
       "0.778   0.058342 -0.014506  0.108054 -0.003132  0.025509  0.065094  0.053748   \n",
       "0.44    0.117398 -0.009584  0.087671  0.010859  0.041583  0.105671  0.070109   \n",
       "0.45   -0.008852  0.001953 -0.003320 -0.000299  0.002016  0.019887  0.046606   \n",
       "3.756   0.044488  0.002086  0.097410  0.005260  0.052662 -0.010282  0.041563   \n",
       "61      0.061387  0.000268  0.107462  0.022081  0.052290  0.090177  0.059680   \n",
       "278     0.089165 -0.022679  0.070119  0.021369  0.002492  0.082089 -0.008344   \n",
       "1       0.126323 -0.030318  0.196840  0.057394  0.241958  0.232741  0.332255   \n",
       "\n",
       "             0.4       0.5       0.6  ...      0.41      0.42      0.43  \\\n",
       "0      -0.003970  0.106241  0.041171  ... -0.026517 -0.021235 -0.033313   \n",
       "0.64   -0.016261 -0.003803  0.032989  ... -0.007270 -0.049802 -0.018516   \n",
       "0.64.1  0.012044  0.093843  0.032135  ... -0.033190 -0.016417 -0.033098   \n",
       "0.1     0.010265 -0.002458 -0.004951  ... -0.000592 -0.012376 -0.007150   \n",
       "0.32    0.029599  0.020824  0.034497  ... -0.032759 -0.046361 -0.026389   \n",
       "0.2     0.079543  0.117416  0.013869  ... -0.019131 -0.008745 -0.015145   \n",
       "0.3     0.044530  0.050767  0.056787  ... -0.033099 -0.051919 -0.027663   \n",
       "0.4     1.000000  0.105285  0.083110  ... -0.027441 -0.032525 -0.019557   \n",
       "0.5     0.105285  1.000000  0.130601  ... -0.014657 -0.031040  0.013591   \n",
       "0.6     0.083110  0.130601  1.000000  ...  0.011933  0.003895  0.007345   \n",
       "0.7     0.128481  0.137742  0.125298  ... -0.032420 -0.055124 -0.025193   \n",
       "0.64.2 -0.002967  0.030352  0.071167  ... -0.027707 -0.030928 -0.044962   \n",
       "0.8     0.026256  0.034717  0.045713  ... -0.023456 -0.051188 -0.028294   \n",
       "0.9     0.012416  0.066829  0.017887  ... -0.019051 -0.005824 -0.014355   \n",
       "0.10    0.072772  0.238426  0.160531  ... -0.018283 -0.002572 -0.003118   \n",
       "0.32.1  0.051121  0.008275  0.025609  ... -0.026838 -0.046570 -0.029557   \n",
       "0.11    0.216408  0.158371  0.081339  ... -0.031554 -0.035935 -0.036702   \n",
       "1.29    0.037875  0.098998  0.036163  ... -0.039466 -0.035682 -0.017377   \n",
       "1.93    0.020649  0.039028  0.093522  ... -0.044309 -0.128869 -0.063822   \n",
       "0.12    0.109155  0.123207  0.030846  ... -0.020856 -0.021450 -0.012077   \n",
       "0.96    0.156914  0.159123  0.098084  ... -0.058656 -0.085170 -0.045465   \n",
       "0.13   -0.016199 -0.019656  0.008191  ...  0.416606 -0.046259 -0.001141   \n",
       "0.14    0.089211  0.126782  0.096788  ... -0.027372 -0.033208 -0.000477   \n",
       "0.15    0.034115  0.099448  0.052113  ... -0.019147 -0.033138 -0.020806   \n",
       "0.16   -0.053058 -0.069955 -0.033561  ...  0.029170  0.136948  0.039712   \n",
       "0.17   -0.041468 -0.049797 -0.013070  ...  0.013548  0.144743  0.064340   \n",
       "0.18   -0.057202 -0.064625 -0.067837  ... -0.022732 -0.028774 -0.017684   \n",
       "0.19   -0.050002 -0.056781  0.019338  ... -0.025028  0.313820  0.031971   \n",
       "0.20   -0.037057 -0.044852 -0.026917  ... -0.018508  0.158580  0.006569   \n",
       "0.21   -0.043418 -0.043660  0.008659  ... -0.019853  0.224174  0.004659   \n",
       "0.22   -0.035825 -0.040170 -0.024436  ... -0.016285  0.233381  0.010713   \n",
       "0.23   -0.034285 -0.033994 -0.015149  ... -0.008858  0.304672  0.013801   \n",
       "0.24   -0.039230 -0.014416 -0.035381  ... -0.005697  0.028637  0.113100   \n",
       "0.25   -0.034820 -0.033612 -0.014446  ... -0.009295  0.303599  0.013682   \n",
       "0.26   -0.035186 -0.041862 -0.020108  ... -0.021599  0.200697  0.034428   \n",
       "0.27   -0.033761 -0.056288 -0.016975  ... -0.018955  0.245435  0.001009   \n",
       "0.28   -0.017484 -0.033267 -0.004970  ...  0.052127  0.107642  0.073000   \n",
       "0.29   -0.012123 -0.002220 -0.017955  ...  0.007884 -0.010437  0.001765   \n",
       "0.30   -0.030403 -0.040857 -0.016106  ...  0.034486  0.107912  0.038469   \n",
       "0.31   -0.005998 -0.009880  0.004148  ... -0.018699  0.268689  0.014058   \n",
       "0.33   -0.003891 -0.035185 -0.025094  ...  0.053030  0.017571  0.034404   \n",
       "0.34   -0.043636 -0.048236 -0.054482  ... -0.007823 -0.013102  0.011086   \n",
       "0.35   -0.004554 -0.034205  0.023184  ...  0.015378  0.060547  0.115542   \n",
       "0.36   -0.030142 -0.035168 -0.026665  ... -0.007261 -0.003217 -0.010738   \n",
       "0.37   -0.002440 -0.075581 -0.032089  ... -0.024709  0.001379  0.008828   \n",
       "0.38   -0.037927 -0.056832 -0.030343  ...  0.015376  0.014741 -0.003174   \n",
       "0.39   -0.006401  0.007516 -0.015552  ...  0.000992 -0.003093 -0.004594   \n",
       "0.40   -0.021230 -0.026025 -0.016851  ... -0.002294 -0.012808 -0.006313   \n",
       "0.41   -0.027441 -0.014657  0.011933  ...  1.000000  0.049108  0.009065   \n",
       "0.42   -0.032525 -0.031040  0.003895  ...  0.049108  1.000000  0.022300   \n",
       "0.43   -0.019557  0.013591  0.007345  ...  0.009065  0.022300  1.000000   \n",
       "0.778   0.031491  0.043685  0.036789  ...  0.020562 -0.030286 -0.031749   \n",
       "0.44    0.057894  0.149347  0.075763  ...  0.006382  0.044689 -0.026411   \n",
       "0.45   -0.008018 -0.000530  0.044822  ...  0.055054  0.023311 -0.006866   \n",
       "3.756   0.011251  0.111306  0.073674  ...  0.003441  0.034361 -0.008181   \n",
       "61      0.037578  0.189252  0.103314  ...  0.040831  0.370978 -0.013992   \n",
       "278     0.040252  0.248726  0.087274  ...  0.055298  0.112212  0.006015   \n",
       "1       0.206915  0.231680  0.139088  ... -0.059597 -0.089551 -0.064678   \n",
       "\n",
       "           0.778      0.44      0.45     3.756        61       278         1  \n",
       "0       0.058342  0.117398 -0.008852  0.044488  0.061387  0.089165  0.126323  \n",
       "0.64   -0.014506 -0.009584  0.001953  0.002086  0.000268 -0.022679 -0.030318  \n",
       "0.64.1  0.108054  0.087671 -0.003320  0.097410  0.107462  0.070119  0.196840  \n",
       "0.1    -0.003132  0.010859 -0.000299  0.005260  0.022081  0.021369  0.057394  \n",
       "0.32    0.025509  0.041583  0.002016  0.052662  0.052290  0.002492  0.241958  \n",
       "0.2     0.065094  0.105671  0.019887 -0.010282  0.090177  0.082089  0.232741  \n",
       "0.3     0.053748  0.070109  0.046606  0.041563  0.059680 -0.008344  0.332255  \n",
       "0.4     0.031491  0.057894 -0.008018  0.011251  0.037578  0.040252  0.206915  \n",
       "0.5     0.043685  0.149347 -0.000530  0.111306  0.189252  0.248726  0.231680  \n",
       "0.6     0.036789  0.075763  0.044822  0.073674  0.103314  0.087274  0.139088  \n",
       "0.7     0.025034  0.070208  0.001119  0.029255  0.086795  0.115056  0.234651  \n",
       "0.64.2  0.013354  0.016730 -0.030442 -0.010001  0.021773  0.020076  0.007711  \n",
       "0.8     0.040781  0.205889 -0.014202 -0.013450  0.041965  0.105150  0.133034  \n",
       "0.9    -0.008476  0.080942  0.006541  0.003021  0.060994  0.169258  0.060085  \n",
       "0.10    0.018634  0.123843 -0.005450  0.017382  0.213995  0.151626  0.195987  \n",
       "0.32.1  0.104253  0.049959  0.035536  0.015037  0.026527  0.003007  0.263236  \n",
       "0.11    0.077097  0.098304 -0.000474  0.038123  0.062676  0.064261  0.263338  \n",
       "1.29    0.039088  0.064042  0.021035 -0.007963  0.075136  0.046390  0.203777  \n",
       "1.93    0.153368  0.091481 -0.002431 -0.030591  0.006529 -0.007307  0.273657  \n",
       "0.12    0.048375  0.034937  0.007210  0.067138  0.099465  0.075751  0.189839  \n",
       "0.96    0.084004  0.141659 -0.004352  0.041067  0.085320  0.051797  0.383265  \n",
       "0.13   -0.004823 -0.011044  0.184425  0.021496  0.027776  0.103954  0.091907  \n",
       "0.14    0.070146  0.310957  0.020133  0.008370  0.123040  0.165978  0.334924  \n",
       "0.15    0.051107  0.104679  0.000698  0.007679  0.044872  0.080993  0.216206  \n",
       "0.16   -0.090822 -0.086658  0.058773 -0.017289 -0.051204 -0.043269 -0.256680  \n",
       "0.17   -0.078331 -0.081219 -0.020698 -0.024238 -0.051804 -0.059603 -0.232928  \n",
       "0.18   -0.067472 -0.068744 -0.020566 -0.025506 -0.054398 -0.096549 -0.183374  \n",
       "0.19   -0.063467 -0.061457 -0.011443 -0.013760 -0.038770 -0.067597 -0.158765  \n",
       "0.20   -0.042309 -0.050243  0.002072 -0.014938 -0.034731 -0.056629 -0.133501  \n",
       "0.21   -0.061666 -0.065491  0.082589 -0.016601 -0.038999 -0.064116 -0.171063  \n",
       "0.22   -0.045253 -0.047486  0.000222 -0.010899 -0.027448 -0.045923 -0.126890  \n",
       "0.23   -0.041511 -0.043494 -0.010738 -0.010500 -0.027730 -0.046797 -0.114195  \n",
       "0.24   -0.048472 -0.048113 -0.009932 -0.015511 -0.025917  0.006919 -0.119904  \n",
       "0.25   -0.038608 -0.039855 -0.010638 -0.002406 -0.024531 -0.044529 -0.112734  \n",
       "0.26   -0.048797 -0.048961 -0.009655 -0.013709 -0.030234 -0.045964 -0.149197  \n",
       "0.27   -0.060349 -0.057951  0.006447 -0.019188 -0.038098 -0.045792 -0.136093  \n",
       "0.28   -0.054537 -0.063918 -0.022645 -0.014427 -0.033201 -0.003490 -0.177990  \n",
       "0.29   -0.015118 -0.012913 -0.003629 -0.006012 -0.009486 -0.013897 -0.031024  \n",
       "0.30   -0.024822 -0.044526 -0.011330 -0.014033 -0.029227 -0.049257 -0.122803  \n",
       "0.31   -0.032486 -0.016737 -0.010666 -0.003947 -0.004833 -0.028806 -0.064762  \n",
       "0.33   -0.025895 -0.036618 -0.011757 -0.008896 -0.023657 -0.026373 -0.097359  \n",
       "0.34   -0.038072 -0.043666 -0.003877 -0.017900 -0.034583 -0.056512 -0.136592  \n",
       "0.35   -0.049336 -0.054713 -0.013930 -0.017683 -0.017277 -0.036529 -0.135632  \n",
       "0.36   -0.033822 -0.036250  0.001164 -0.013158 -0.025917 -0.040661 -0.094576  \n",
       "0.37    0.067613 -0.049388 -0.023885 -0.026982 -0.051855 -0.095445 -0.140353  \n",
       "0.38   -0.028820 -0.050123 -0.015044 -0.017410 -0.033363 -0.046372 -0.146110  \n",
       "0.39   -0.017670 -0.018554  0.000307 -0.006465 -0.010153  0.005158 -0.044667  \n",
       "0.40   -0.026562 -0.030758 -0.008577 -0.008115 -0.016893 -0.010033 -0.084004  \n",
       "0.41    0.020562  0.006382  0.055054  0.003441  0.040831  0.055298 -0.059597  \n",
       "0.42   -0.030286  0.044689  0.023311  0.034361  0.370978  0.112212 -0.089551  \n",
       "0.43   -0.031749 -0.026411 -0.006866 -0.008181 -0.013992  0.006015 -0.064678  \n",
       "0.778   1.000000  0.142962  0.020939  0.054317  0.077389  0.036324  0.241771  \n",
       "0.44    0.142962  1.000000  0.012606  0.079995  0.183149  0.201949  0.323769  \n",
       "0.45    0.020939  0.012606  1.000000  0.013496  0.061659  0.042568  0.065105  \n",
       "3.756   0.054317  0.079995  0.013496  1.000000  0.492639  0.162314  0.110030  \n",
       "61      0.077389  0.183149  0.061659  0.492639  1.000000  0.475486  0.216121  \n",
       "278     0.036324  0.201949  0.042568  0.162314  0.475486  1.000000  0.249208  \n",
       "1       0.241771  0.323769  0.065105  0.110030  0.216121  0.249208  1.000000  \n",
       "\n",
       "[58 rows x 58 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62fb5774-ebec-472c-b42f-20eb898887b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "0.64      0\n",
       "0.64.1    0\n",
       "0.1       0\n",
       "0.32      0\n",
       "0.2       0\n",
       "0.3       0\n",
       "0.4       0\n",
       "0.5       0\n",
       "0.6       0\n",
       "0.7       0\n",
       "0.64.2    0\n",
       "0.8       0\n",
       "0.9       0\n",
       "0.10      0\n",
       "0.32.1    0\n",
       "0.11      0\n",
       "1.29      0\n",
       "1.93      0\n",
       "0.12      0\n",
       "0.96      0\n",
       "0.13      0\n",
       "0.14      0\n",
       "0.15      0\n",
       "0.16      0\n",
       "0.17      0\n",
       "0.18      0\n",
       "0.19      0\n",
       "0.20      0\n",
       "0.21      0\n",
       "0.22      0\n",
       "0.23      0\n",
       "0.24      0\n",
       "0.25      0\n",
       "0.26      0\n",
       "0.27      0\n",
       "0.28      0\n",
       "0.29      0\n",
       "0.30      0\n",
       "0.31      0\n",
       "0.33      0\n",
       "0.34      0\n",
       "0.35      0\n",
       "0.36      0\n",
       "0.37      0\n",
       "0.38      0\n",
       "0.39      0\n",
       "0.40      0\n",
       "0.41      0\n",
       "0.42      0\n",
       "0.43      0\n",
       "0.778     0\n",
       "0.44      0\n",
       "0.45      0\n",
       "3.756     0\n",
       "61        0\n",
       "278       0\n",
       "1         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "763cbda4-1b7e-4d3e-b0d7-e3385ce0ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fbdbbe3-4b60-4d33-8591-7e2fd1def3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependent and independent target\n",
    "x= df.iloc[:,:-1].values\n",
    "y =df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b28cf32-a503-4b63-bcf5-ce4fb07545ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.100e-01, 2.800e-01, 5.000e-01, ..., 5.114e+00, 1.010e+02,\n",
       "        1.028e+03],\n",
       "       [6.000e-02, 0.000e+00, 7.100e-01, ..., 9.821e+00, 4.850e+02,\n",
       "        2.259e+03],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 3.537e+00, 4.000e+01,\n",
       "        1.910e+02],\n",
       "       ...,\n",
       "       [3.000e-01, 0.000e+00, 3.000e-01, ..., 1.404e+00, 6.000e+00,\n",
       "        1.180e+02],\n",
       "       [9.600e-01, 0.000e+00, 0.000e+00, ..., 1.147e+00, 5.000e+00,\n",
       "        7.800e+01],\n",
       "       [0.000e+00, 0.000e+00, 6.500e-01, ..., 1.250e+00, 5.000e+00,\n",
       "        4.000e+01]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89e8c5f7-5898-4795-9955-b2b3b40839d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8d592d2-3948-47c5-9ae3-33b6a2322f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifiers\n",
    "\n",
    "bernoulli_clf = BernoulliNB()\n",
    "multinomial_clf = MultinomialNB()\n",
    "gaussian_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6391db-096b-417e-9a5c-35c7e3902732",
   "metadata": {},
   "source": [
    "# bernoulli nb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7dda2c7-1803-45c4-aeb2-aa42e0be8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "scoring = ['precision_macro','recall_macro','accuracy','f1']\n",
    "scores = cross_validate(bernoulli_clf,x,y,scoring=scoring,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4faface4-4f74-4d0b-b6f5-460e4d6f3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of fold 1:0.8847826086956522\n",
      "accuracy of fold 2:0.9152173913043479\n",
      "accuracy of fold 3:0.9021739130434783\n",
      "accuracy of fold 4:0.908695652173913\n",
      "accuracy of fold 5:0.8913043478260869\n",
      "accuracy of fold 6:0.9282608695652174\n",
      "accuracy of fold 7:0.9260869565217391\n",
      "accuracy of fold 8:0.8913043478260869\n",
      "accuracy of fold 9:0.808695652173913\n",
      "accuracy of fold 10:0.782608695652174\n",
      "average accuracy across all folds:0.8839130434782609\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"accuracy of fold {i+1}:{scores['test_accuracy'][i]}\")\n",
    "print(f\"average accuracy across all folds:{np.mean(scores['test_accuracy'])}\")\n",
    "print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab5fadda-3b1d-4698-9e37-0a256a4f6106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of fold 1:0.8973111202227066\n",
      "precision of fold 2:0.9158117323296708\n",
      "precision of fold 3:0.9153763440860215\n",
      "precision of fold 4:0.9114535182331793\n",
      "precision of fold 5:0.892552645095018\n",
      "precision of fold 6:0.9259502749223045\n",
      "precision of fold 7:0.9316890789283427\n",
      "precision of fold 8:0.8989952406134321\n",
      "precision of fold 9:0.7994194484760523\n",
      "precision of fold 10:0.7721867321867322\n",
      "average precision across all folds: 0.886074613509346\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f\"precision of fold {i+1}:{scores['test_precision_macro'][i]}\")\n",
    "print(f\"average precision across all folds: {np.mean(scores['test_precision_macro'])}\")\n",
    "print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ce5a1df9-89aa-4e01-9391-723ab5098859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall of fold 1:0.8638825203573405\n",
      "recall of fold 2:0.9061388252035734\n",
      "recall of fold 3:0.8824828214420087\n",
      "recall of fold 4:0.8965920117230044\n",
      "recall of fold 5:0.8783738291847363\n",
      "recall of fold 6:0.9233945226638152\n",
      "recall of fold 7:0.9138398780173864\n",
      "recall of fold 8:0.8735222479653062\n",
      "recall of fold 9:0.8063922057862531\n",
      "recall of fold 10:0.774213350759421\n",
      "average recall across all folds:0.8718832213102845\n",
      "__________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f\"recall of fold {i+1}:{scores['test_recall_macro'][i]}\")\n",
    "print(f\"average recall across all folds:{np.mean(scores['test_recall_macro'])}\")\n",
    "print('__________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95c4387d-336e-4f18-bce4-cba489bbf78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of fold 1: 0.8398791540785498\n",
      "f1 score of fold 2: 0.8895184135977338\n",
      "f1 score of fold 3: 0.8640483383685801\n",
      "f1 score of fold 4: 0.8786127167630058\n",
      "f1 score of fold 5: 0.8554913294797687\n",
      "f1 score of fold 6: 0.9080779944289694\n",
      "f1 score of fold 7: 0.9011627906976744\n",
      "f1 score of fold 8: 0.851190476190476\n",
      "f1 score of fold 9: 0.7659574468085106\n",
      "f1 score of fold 10: 0.726775956284153\n",
      "average f1 score across all folds : 0.8480714616697421\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(f\"f1 score of fold {i+1}: {scores['test_f1'][i]}\")\n",
    "print(f\"average f1 score across all folds : {np.mean(scores['test_f1'])}\")\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8a841-945a-4929-9ce2-b5746b0b2e73",
   "metadata": {},
   "source": [
    "# gaussian nb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "baec6a73-cb5e-4362-805f-6092fa5fdea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of fold 1 : 0.8434782608695652\n",
      "accuracy of fold 2 : 0.8630434782608696\n",
      "accuracy of fold 3 : 0.8782608695652174\n",
      "accuracy of fold 4 : 0.8673913043478261\n",
      "accuracy of fold 5 : 0.8847826086956522\n",
      "accuracy of fold 6 : 0.8282608695652174\n",
      "accuracy of fold 7 : 0.8326086956521739\n",
      "accuracy of fold 8 : 0.8673913043478261\n",
      "accuracy of fold 9 : 0.6347826086956522\n",
      "accuracy of fold 10 : 0.717391304347826\n",
      "average accuracy across all folds: 0.8217391304347826\n",
      "--------------------\n",
      "precision of fold 1 : 0.8486742424242424\n",
      "precision of fold 2 : 0.8665588162948668\n",
      "precision of fold 3 : 0.8778048734380612\n",
      "precision of fold 4 : 0.8704222154963681\n",
      "precision of fold 5 : 0.8792075070283414\n",
      "precision of fold 6 : 0.8447074142156863\n",
      "precision of fold 7 : 0.8430462568472307\n",
      "precision of fold 8 : 0.8634469696969698\n",
      "precision of fold 9 : 0.7185854544618426\n",
      "precision of fold 10 : 0.7424239475271999\n",
      "average precision across all folds: 0.8354877697430808\n",
      "-----------------\n",
      "recall of fold 1:0.8638627559490869\n",
      "recall of fold 2:0.88289588109732\n",
      "recall of fold 3:0.8957603120853879\n",
      "recall of fold 4:0.8877700548525713\n",
      "recall of fold 5:0.8953147587080932\n",
      "recall of fold 6:0.8564823065803283\n",
      "recall of fold 7:0.8571555872393513\n",
      "recall of fold 8:0.8800075249014832\n",
      "recall of fold 9:0.6882512525000495\n",
      "recall of fold 10:0.7466484484841284\n",
      "average recall across all folds: 0.84541488823978\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "scoring = ['precision_macro','recall_macro','accuracy','f1']\n",
    "scores = cross_validate(gaussian_clf,x,y,scoring=scoring,cv=10)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"accuracy of fold {i+1} : {scores['test_accuracy'][i]}\")\n",
    "print(f\"average accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print('--------------------')\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"precision of fold {i+1} : {scores['test_precision_macro'][i]}\")\n",
    "print(f\"average precision across all folds: {np.mean(scores['test_precision_macro'])}\")\n",
    "print('-----------------')\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"recall of fold {i+1}:{scores['test_recall_macro'][i]}\")\n",
    "print(f\"average recall across all folds: {np.mean(scores['test_recall_macro'])}\")\n",
    "print('--------------')          \n",
    "          \n",
    "    \n",
    "for i in range(0,10):\n",
    "    print(f\"f1 score of fold {i+1} : {scores['test_f1'][i]}\")\n",
    "print(f\"average f1 score across all folds: {np.mean(scores['test_f1'])}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965682fa-e2bc-412b-8493-c70aff1fb63c",
   "metadata": {},
   "source": [
    "# multinomial nb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d243114f-3533-4cdb-9695-0bb90ff4d0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of fold 1: 0.7913043478260869\n",
      "accuracy of fold 2: 0.7891304347826087\n",
      "accuracy of fold 3: 0.8108695652173913\n",
      "accuracy of fold 4: 0.8347826086956521\n",
      "accuracy of fold 5: 0.8282608695652174\n",
      "accuracy of fold 6: 0.7782608695652173\n",
      "accuracy of fold 7: 0.7782608695652173\n",
      "accuracy of fold 8: 0.8130434782608695\n",
      "accuracy of fold 9: 0.6934782608695652\n",
      "accuracy of fold 10: 0.7434782608695653\n",
      "average accuracy across all folds: 0.786086956521739\n",
      "-------------------\n",
      "precision of fold 1:0.7842799770510613\n",
      "precision of fold 2:0.784690252464737\n",
      "precision of fold 3:0.8042645140247879\n",
      "precision of fold 4:0.8276337066538899\n",
      "precision of fold 5:0.8230185909980431\n",
      "precision of fold 6:0.7678085051392671\n",
      "precision of fold 7:0.7676658476658477\n",
      "precision of fold 8:0.8081634339303051\n",
      "precision of fold 9:0.6963907384987893\n",
      "precision of fold 10:0.7314987714987715\n",
      "average precision across all folds:0.77954143379255\n",
      "--------------------------\n",
      "recall of fold 1 : 0.7742114001106807\n",
      "recall of fold 2 : 0.7676693809787335\n",
      "recall of fold 3 : 0.7955702093110755\n",
      "recall of fold 4 : 0.8249866333986812\n",
      "recall of fold 5 : 0.8137883918493436\n",
      "recall of fold 6 : 0.7667478563931959\n",
      "recall of fold 7 : 0.769658805124854\n",
      "recall of fold 8 : 0.7954216915186438\n",
      "recall of fold 9 : 0.7055882294698905\n",
      "recall of fold 10 : 0.7332224400483178\n",
      "average recall across all folds: 0.77954143379255\n",
      "_----------------------\n",
      "f1 score of fold 1: 0.7241379310344828\n",
      "f1 score of fold 2: 0.7138643067846608\n",
      "f1 score of fold 3: 0.7507163323782237\n",
      "f1 score of fold 4: 0.7877094972067039\n",
      "f1 score of fold 5: 0.7736389684813754\n",
      "f1 score of fold 6: 0.7166666666666666\n",
      "f1 score of fold 7: 0.7213114754098361\n",
      "f1 score of fold 8: 0.75\n",
      "f1 score of fold 9: 0.6618705035971223\n",
      "f1 score of fold 10: 0.6775956284153005\n",
      "average f1 score across all folds: 0.7277511309974372\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "scoring = ['precision_macro','recall_macro','accuracy','f1']\n",
    "scores = cross_validate(multinomial_clf,x,y,scoring=scoring,cv=10)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# scores for each fold\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"average accuracy across all folds: {np.mean(scores['test_accuracy'])}\")    \n",
    "print('-------------------')    \n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"precision of fold {i+1}:{scores['test_precision_macro'][i]}\")\n",
    "print(f\"average precision across all folds:{np.mean(scores['test_precision_macro'])}\")\n",
    "print('--------------------------')\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"recall of fold {i+1} : {scores['test_recall_macro'][i]}\")\n",
    "print(f\"average recall across all folds: {np.mean(scores['test_precision_macro'])}\")\n",
    "print('_----------------------')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"f1 score of fold {i+1}: {scores['test_f1'][i]}\")\n",
    "print(f\"average f1 score across all folds: {np.mean(scores['test_f1'])}\")\n",
    "print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051ab12-018d-4340-852e-8e890174c424",
   "metadata": {},
   "source": [
    "# Discussion about scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49058097-d504-4b93-8a9e-cd3e726810ea",
   "metadata": {},
   "source": [
    "- comparing the average f1 score across all the folds for the three classifiers we can see that bernoulli naive bayes classifier \n",
    "has the best performance on the data;\n",
    "\n",
    "- but during the lecture we discussed that bernoulli is used in cases where the input\n",
    "features are binary ,but given dataset are not in binary \n",
    "\n",
    "- after research on internet i found that there could be certain circumstances where bernoulli\n",
    "nb may perform better than guassian nb or multinomial nb on this type of dataset.\n",
    "\n",
    "- one possible explaination is that the data may have binary like patterns, such as containing\n",
    "many exact 0's or 1's ,or having many features that take on only a few discrete values,\n",
    "which bernoulli naive bayes can exploit.\n",
    "\n",
    "so, let go ahead with bernoulli naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a55da332-d66f-4b7d-a8d0-0b2ecfbb806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.33,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "890021f6-c38f-4e64-8ae6-33abd8e6e7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1a7f80b4-c75e-4843-ba80-0ec26b53af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bernoulli_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e5ee498c-4fe9-4de0-b23a-29ee5315512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "70842c87-2811-4113-875d-ff22601b4b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       885\n",
      "           1       0.88      0.80      0.84       633\n",
      "\n",
      "    accuracy                           0.87      1518\n",
      "   macro avg       0.87      0.86      0.87      1518\n",
      "weighted avg       0.87      0.87      0.87      1518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7d217ed2-6dde-4ff4-9b7b-a39baaa4d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm= confusion_matrix(y_test,y_pred,labels=bernoulli_clf.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels = bernoulli_clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f9f1f-8da3-4fc4-b875-70f6728000d8",
   "metadata": {},
   "source": [
    "# Summary of confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa53de4d-0a1e-41c1-b088-db6d6be3fe59",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3213666384.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[110], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "- The confusion matrix indicates that the model has correctly predicted the majority of instances for both\n",
    "classes,with a higher accuracy in predicting class1 . However,it has made some incorrect predictions ,with a higher number of false negative for class 0,and \n",
    "false positive for class1.\n",
    "\n",
    "\n",
    "# suggestion for future work\n",
    "\n",
    "- this is a very basic implementation of training a classifier on the given data.\n",
    "there area a lot of improvements that could be made some of them are as follows:\n",
    "    1 perform hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53620c5f-3d15-4a73-9b96-8efce989f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
