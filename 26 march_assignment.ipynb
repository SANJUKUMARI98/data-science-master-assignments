{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918a3da4-34f8-44de-975f-a796429ee30f",
   "metadata": {},
   "source": [
    "## 26 march assignment (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0417b6-267e-45fc-99de-6dbd2c63e0f6",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c254e5a-cb0c-4853-b956-c96f2628c448",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b0555ed-453b-4e0b-aa1b-f751a28ba3e9",
   "metadata": {},
   "source": [
    "Simple linear regression is a stastical method used to establish a linear relationship\n",
    "between two variables where one variable is considered independent (predictor variable) and the other variable is considered  dependent(response variale).\n",
    "The relationship between the two variables is represented by a straight line.The formula for simple linear regression is Y= a+bX, where Y is the dependent variable, X is the\n",
    "independent variable,a is the y-intercept, b is the slope of the line.\n",
    "For example, simple linear regression can be used to determine the relationship between the\n",
    "height  of a person and their weight . Here ,the height is the predictor variable, and the weight is the response variable.\n",
    "The simple linear regression equation for this relationship would be weight = a+b (height)\n",
    "\n",
    "Multiple linear regression is a statistical method used to establish the relationship between two or more independent variables and a dependent variable. The formula for multiple linear regression is Y=a+b1X1+b2X2+....+bnXn, where Y is  dependent variable,X1, X2,X3,....Xn are the independent variables,a is the the y-intercept ,and b1,b2,b3...bn are the slopes of the lines.\n",
    "for  example,multiple linear regression can be used to  determine the relationship between the sales of a product\n",
    "and the price,advertising  budget ,and seasonality.Here ,price ,advertising budget, and seasonality are the independent variables,and\n",
    "sales are the dependent variables. The multiple linear regression equation for this relationship would  be sales=a+b1(price)+b2(advertising budget)+b3(seasonality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891e46b-1052-46af-b314-82795f5498cc",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b746f8-c609-43ac-bb11-8b85fd103eec",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02b203af-ea17-4d65-b998-ae4b64f6117f",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used to establish a relationship between two variables.\n",
    "However, linear regression assumes certain conditions that need to be met for  the results to be accurate reliable. The following are the main assumptions of linear regression:\n",
    "    1. Linearity:Linear  regression assumes that there is a linear relationship between the dependent variable  and the independent variables.YOu can check the linearity\n",
    "    assumption by plotting  the data and checking whether the points lie close to a straight line.\n",
    "    \n",
    "    2. Independence: Linear regression assumes that the observations are independent of each other.In\n",
    "    other words, the value of one observation does not depend on the value of another observation YOu can check the independence assumption by examining the  data\n",
    "    collection process and ensuring that there is no correlation between the observations.\n",
    "    3. Homoscedasticity: Linear regression assumes that the variance of the residuls is constant across all levels of the independent variable.YOu can check homoscedasicity  assumption by plotting the residuals against the predicted values and checking for a \n",
    "    consistent spread of points.\n",
    "    4.Normality: Linear regression assumes that the residuals are normally distributed. YOu can check the normality assumption  by plotting a histogram or a Q-Q plot of  the residuals and ensuring that they follow a normal distribution.\n",
    "    \n",
    "    To check whether these assumptions hold in a given dataset, you can use various diagnostic tools such as residual plots, normal probability plots,and leverage plots.\n",
    "    These plots can help you identify any violatios of the assumptions and take appropriate  corrective  measures, such as transforming the data or using a different statistical method. Additionlly, statistical tests such as Shapiro-Wilk test can be used to test the normality assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154692b9-1a43-4368-b83d-53838155d334",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34358866-522e-40c8-a0cb-8bc1d5f643c5",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85107f98-ef6c-4597-aafa-5467916fe759",
   "metadata": {},
   "source": [
    "In a  linear regression model,the slope and intercept are key parameters that describe the relationship between\n",
    "the dependent variable and the  independent variable.\n",
    "\n",
    "The intercept (often denoted by 'a' or 'b0') is the value of the dependent variable when the independent  variable\n",
    "for every one-unit change in the independent variable. It reflects the steepness of  the regression line.\n",
    "\n",
    "The slope (often denoted by 'b' or 'beta') represents the change in the dependent variable for every one -unit change in the independent variable.It reflects the steepness of\n",
    "the  regression line.\n",
    "For example,suppose we have  data on the salaries of employees and their years of  experience. We fit a linear regression model to predict salary\n",
    "(dependent variable)from years of experience (independent variable).The regression equation is Salary= a+b(experience).\n",
    "\n",
    "If the intercept (a) is 10,000,it means that a employee with zero years of experience (Experience=0)would earn a starting salary or $10,000.\n",
    "If the slope (b) is 5,000, it means that for every one-year increase in experience,the salary is expected to increse by$5,000.\n",
    "\n",
    "So,if an employee has five years of experience , the predicted salary would be $35,000(10,000+5,000*5).\n",
    "Thus, in this scenario,the intercept and slope of linear regression model provide us with valuable information about the \n",
    "expected starting salary and the rate of increase in salary with experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84cd8-1882-4b61-8399-8d234278867d",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bf796-6aff-4f1f-be74-dd719ef59e2e",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af66c200-d138-48b3-8ae9-e2c088818a06",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost function of a machine learning model.\n",
    "The cost function is a measure of the differnce between the predicted values and the actual values of the dependent variable in the training dataset. \n",
    "The goal of gradient descent is to adjust the parameters of the machine learning model to find the minimum of the cost function.\n",
    "\n",
    "The basic idea behind gradient descent is to iteratively adjust the parameters of the model in the direction of the steepest descent of the cost function. In other words,the\n",
    "algorithm calculates the gradient of the cost function with respect to each parameter of the model and adjust the parameters in the opposite direction of  the gradient.\n",
    "\n",
    "The gradient descent algorithm can be summarized in the following steps:\n",
    "    1. Initialize the parameters of the model with random values.\n",
    "    2. Calculate the cost function for the current values of the parameters.\n",
    "    3. Calculate the gradient of  the cost function with respect to each parameter.\n",
    "    4. Adjust the parameters in the opposite  direction of the gradient ,using a learning rate(a hyperparameter that controls the step size of the algorithm ).\n",
    "    5. Repeat steps 2-4 until the cost function reaches a minimum or a predefined number of iterations is reached. \n",
    "    \n",
    "    Gradient descent is used in various machine learning algorithms, including linear regression,logistic regression,and artificial neural  networks.By \n",
    "    iteratively adjusting the parameters of the model to minimize the cost function ,gradient descent helps the model learn the underlying patterns in the data  and make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa6c53-f8dd-414f-ad6e-a52ce8889cf9",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9e71a-1f29-44e4-a2b8-2ce2714d3767",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6672b0b1-ec62-491f-8cea-d1525f2f13c3",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical method used to establish a relationship between a dependent variable and \n",
    "multiple independent variables. It is an extension of simple linear regression,which involves only one independent variable.\n",
    "\n",
    "The multiple linear regression model can be written as:\n",
    "    y=b0+b1x1+b2x2+...bnxn+e\n",
    "    \n",
    "    where y is the dependent variable,x1,x2,x3.....xn are the independent variables, b0 is the intercept,b1,b2,b3...bn  are the coefficients (or slopes) of the independent variables,and e is the error term.\n",
    "    \n",
    "    The goal of multiple linear regression is to estimate the values of the coefficients the best fit the data, i.e., that minimize the sum of squared difference between the predicted values and the actual values of the dependent variable.\n",
    "    The main difference between multiple linear regression  and simple linear regression is the number of independent variables. In simple linear regression , there is only one independent variable,while in multiple  linear regression , there are multiple independent variables.\n",
    "    \n",
    "    Another important difference is that in multiple linear regression ,the interpretation of the  coefficients is more complex.The coefficient of\n",
    "    each independent variable represents the change in the dependent variable for a one-unit change in that independent variable, holding all other independent variables constant.\n",
    "    \n",
    "    For example,in a multiple linear regression model that predicts housing prices based on the size\n",
    "    of the house, the number of bedrooms, and the location,the coefficient of the size variable repesents the change in the price for a one -unit increase in size,holding\n",
    "    the number of bedrooms variable represents the change in the price of a one-unit increase in the number of bedrooms, holding the size and location constant. \n",
    "    \n",
    "    In summary , multiple linear regression is a more complex and flexible statistical model that can capture the relationship between a dependent variable and multiple independent variables,allowing for more accurate and \n",
    "    nuanced predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4a72e-3a71-4c52-988f-87b635ba01a9",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823aee-d6e6-4fed-ad57-77ebf10a71b8",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce71a038-ee5b-4e3c-bac5-c1090b118f51",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon in multiple linear regression where is a high degree of correlation between two or more predictor variables.\n",
    "In other words,it occurs when there are two or more independent variables that are highly correlated with each other, making it difficult to distinguish their individual effects on the dependent variable.\n",
    "\n",
    "Detecting Multicollinearity: \n",
    "    - Correlation Matrix: One way to detect multicollinearity is to examine the correlation matrix between the predictor variables. High correlation coefficients indicate a strong correlation between two variables.\n",
    "    - Variance inflation factor(VIF): Another  way to detect multicollinearity is to  calculate the VIF for each\n",
    "    predictor variable. A VIF  value of 1 indicates no multicollinearity, while values above 1 indicate increasing levels of multicollinearity. A \n",
    "    VIF value of 5 or higher is generally considered to  be a sign of  significant \n",
    "    multicollinearity.\n",
    "    \n",
    "    Addressing Multicollinearity\n",
    "    \n",
    "    - Drop one of the correlated variables: If two or more predictor variables are highly correlated , one of the \n",
    "    them can be dropped from  the regression model. This can reduce the multicollinearity  in the model.\n",
    "    - Combine correlated variables:Another option is to combine the correlated variables into a single variable,such as calculating the mean of the two variables or creating an index.\n",
    "    \n",
    "    - Ridge regression: A more advanced technique is to use ridge regression ,which adds a penalty term to the regression model to reduce the impact of \n",
    "    multicollinearity . This can improve the stability and accuracy of the regression coefficients.\n",
    "    \n",
    "    Overall, it's important to detect and address multicollinearity in multiple linear regression models to ensure that results are accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d0634-6c4e-40ec-b59b-64443bffa12c",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339bcb3-9ac8-464e-bf3e-fedf201b164f",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ba9eb3b-3815-44ce-9fb0-f4f8ea5c8120",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that models the relationship between \n",
    "the independent variable(X) and the dependent variable(Y) as a nth degree polynomial . In other words,it allows us to model non-linear relationships between X and Y by fitting a curve to data rather than\n",
    "a straight  line.\n",
    "\n",
    "The polynomial regression equation can be written as:\n",
    "    Y=B0+B1X+B2X^2+.....BnX^n +e\n",
    "    \n",
    "    where B0,B1,B2,.....,Bn are the regression coefficients,X is the independent  variable ,e,is\n",
    "    the error term, and n is the degree of the polynomial.\n",
    "    \n",
    "The key difference between linear regression and polynomial regression is that linear regression\n",
    "models the relationship between X and Y as a straight line, while polynomial regression models it as a curve.\n",
    "\n",
    "Linear regression equation:   Y=B0+B1X+B2X^2+....+BnX^n+e\n",
    "\n",
    "In linear regression ,the relationship between X and Y is assumed  to be linear ,which means that a change in X results in a proportional change in Y. However , in many real-world scenarios, the relationship between X and Y is not  linear,and a curve may be a better fit for the data. This is where polynomial regression comes in-it allows us to model non-linear relationships between X and Y.\n",
    "\n",
    "Overall , polynomial regression is a powerful tool for modelling non-linear relationship between variables\n",
    "and  can be used in a variety of fields, including engineering,economics,and social sciences. However, it's important to be cautious when using polynomial \n",
    "regression and to ensure that the degree of the polynomial is appropriate for the data  and does not overfit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e7539-f8a4-4ac1-8a6b-967d1a49366c",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9c408-59ef-469b-a14f-d38bd3e13409",
   "metadata": {},
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "624821d5-6233-4b7a-b6f7-6d8d5e93a828",
   "metadata": {},
   "source": [
    "Polynomial regression is an extension of linear regression that allows for modelling  nonlinear \n",
    "relationship between the dependent variable and one or more independent variables. The main \n",
    "advantages and disadvantages of polynomial regression compared to linear regression are:\n",
    "    \n",
    "    advantages:\n",
    "        \n",
    "        1 Better fit: Polynomial regression can fit more complex curves and can capture more nuanced relationship between variables\n",
    "        compared to linear regression. \n",
    "        2 Flexibility: Polynomial regression can be used for modelling non linea \n",
    "        relationship ,whilw linear regressioni is restricted to linear relationships.\n",
    "        3 Interpretation: The coefficients in polynomial regression can be interpreted in a similar way to linear regression ,which can help to explain the relationship between variables.\n",
    "        \n",
    "  disadvantages:\n",
    "    \n",
    "    1. overfitting: Polynomial regression models can easily overfit the data, leading to poor generalization performance on \n",
    "    new data.\n",
    "    2. Complexity: Polynomial regression models are more complex than linear regression models, which can make them more difficult to interpret and understand.\n",
    "    3.Extrapolation: Polynomial regression models can be sensitive to extrapolation,meaning that the model may not perform well when predicting values outside of the\n",
    "    range of the training data.\n",
    "    In situations where the relationship between the independent and dependent variables is nonlinear, polynomial regression can be a better choice than linear  regression . \n",
    "    Additionally, polynomial regression can be useful when there are multiple independent variables that interact in a nonlinear way. However,it is important to be careful when using polynomial  regression,as\n",
    "    it can be prone to overfitting and may not perform well when \n",
    "    extrapolating outside of the range of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702be113-6811-4075-afdc-d1de72758c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
