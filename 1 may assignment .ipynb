{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41ea42f-f2ba-40d6-8ee4-cf9165c19964",
   "metadata": {},
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "\n",
    "ans\n",
    "\n",
    "A contingency matrix, also known as a confusion matrix or an error matrix, is a table that is used to evaluate the performance of a classification model, especially in binary classification tasks. It provides a summary of the classification results by comparing the predicted class labels to the actual class labels in a tabular format. The contingency matrix is a valuable tool for assessing the performance of a classifier and calculating various evaluation metrics.\n",
    "\n",
    "A typical contingency matrix has four entries:\n",
    "\n",
    "True Positives (TP): This represents the number of instances that were correctly predicted as positive (belonging to the positive class) by the classifier.\n",
    "\n",
    "False Positives (FP): This represents the number of instances that were incorrectly predicted as positive when they actually belong to the negative class (Type I error).\n",
    "\n",
    "True Negatives (TN): This represents the number of instances that were correctly predicted as negative (belonging to the negative class) by the classifier.\n",
    "\n",
    "False Negatives (FN): This represents the number of instances that were incorrectly predicted as negative when they actually belong to the positive class (Type II error).\n",
    "\n",
    "The arrangement of these values in the contingency matrix looks like this:\n",
    "\n",
    "mathematica\n",
    "Copy code\n",
    "                  Actual Positive    Actual Negative\n",
    "Predicted Positive       TP                FP\n",
    "Predicted Negative       FN                TN\n",
    "How a Contingency Matrix is Used to Evaluate Model Performance:\n",
    "\n",
    "Accuracy: The contingency matrix is used to calculate accuracy, which is a measure of how many predictions the classifier got right out of all predictions. It is calculated as (TP + TN) / (TP + FP + FN + TN).\n",
    "\n",
    "Precision (Positive Predictive Value): Precision is a measure of how many of the predicted positive instances were actually positive. It is calculated as TP / (TP + FP).\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall is a measure of how many of the actual positive instances were correctly predicted as positive. It is calculated as TP / (TP + FN).\n",
    "\n",
    "F1-Score: The F1-Score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when dealing with imbalanced datasets. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures how many of the actual negative instances were correctly predicted as negative. It is calculated as TN / (TN + FP).\n",
    "\n",
    "False Positive Rate: The false positive rate is the proportion of actual negative instances that were incorrectly predicted as positive. It is calculated as FP / (TN + FP).\n",
    "\n",
    "False Negative Rate: The false negative rate is the proportion of actual positive instances that were incorrectly predicted as negative. It is calculated as FN / (TP + FN).\n",
    "\n",
    "Sensitivity (Recall) vs. Specificity Trade-Off: By examining the values in the contingency matrix, you can make decisions about adjusting the model's threshold to optimize either sensitivity or specificity based on the problem's requirements.\n",
    "\n",
    "In summary, a contingency matrix is a fundamental tool for evaluating the performance of a classification model by comparing its predictions to the actual ground truth. It allows you to calculate various performance metrics that provide insights into the model's strengths and weaknesses, depending on the specific goals and requirements of the classification task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?\n",
    "\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A pair confusion matrix, also known as a pairwise confusion matrix or a multilabel confusion matrix, is an extension of the regular confusion matrix used in multi-label classification tasks. While a regular confusion matrix is primarily used for traditional binary or multi-class classification, a pair confusion matrix is designed to handle scenarios where each instance can belong to multiple classes or labels simultaneously.\n",
    "\n",
    "Here are the key differences between a pair confusion matrix and a regular confusion matrix:\n",
    "\n",
    "Regular Confusion Matrix (Binary or Multi-class Classification):\n",
    "\n",
    "In binary classification, it typically consists of four entries: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "In multi-class classification, it extends to cover more classes, but it still counts only one true class label for each instance.\n",
    "It assumes that each instance belongs to one and only one class or label.\n",
    "Pair Confusion Matrix (Multi-label Classification):\n",
    "\n",
    "In multi-label classification, each instance can belong to multiple classes or labels simultaneously. A pair confusion matrix accommodates this by allowing for multiple true class labels for each instance.\n",
    "It includes various combinations of TP, TN, FP, and FN for each possible label or class assignment for an instance.\n",
    "It considers all possible label assignments for each instance and counts the relevant confusion matrix values accordingly.\n",
    "Usefulness of Pair Confusion Matrix:\n",
    "\n",
    "A pair confusion matrix is useful in multi-label classification situations for several reasons:\n",
    "\n",
    "Handling Multi-label Scenarios: In many real-world applications, instances can have multiple labels or belong to multiple categories simultaneously. For example, in image classification, an image may contain multiple objects, each associated with a different label. Pair confusion matrices can capture the performance of a multi-label classifier more accurately.\n",
    "\n",
    "Evaluation and Metrics: It allows for the calculation of metrics specific to multi-label classification, such as precision, recall, F1-score, and Hamming loss, which consider the multi-label nature of the problem. These metrics help assess the model's ability to correctly predict the presence or absence of multiple labels for each instance.\n",
    "\n",
    "Performance Analysis: It provides insights into the model's performance for each individual label or class. This can be particularly important when certain labels are more critical or have different importance levels than others in a multi-label classification task.\n",
    "\n",
    "Threshold Selection: In multi-label classification, models often produce probability scores for each label. The pair confusion matrix can help you choose an appropriate probability threshold for making label predictions based on the trade-off between precision and recall for each label.\n",
    "\n",
    "In summary, a pair confusion matrix is a valuable tool for evaluating the performance of multi-label classification models. It accounts for the fact that instances can belong to multiple classes simultaneously and allows for a more detailed analysis of the model's performance across various labels or classes. It is particularly useful when dealing with complex classification scenarios where instances can have multiple, overlapping labels or categories.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?\n",
    "\n",
    "\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "In the context of natural language processing (NLP) and machine learning, extrinsic measures (also known as task-based or application-oriented measures) are evaluation metrics that assess the performance of language models or NLP systems by measuring their effectiveness in solving real-world tasks or applications. These measures are based on the performance of the model when applied to specific downstream tasks, rather than evaluating the model's performance in isolation.\n",
    "\n",
    "Here's how extrinsic measures are typically used to evaluate the performance of language models:\n",
    "\n",
    "Defining Downstream Tasks: Researchers or practitioners define specific downstream NLP tasks or applications they want to evaluate. These tasks can include sentiment analysis, machine translation, text summarization, question-answering, named entity recognition, and many others.\n",
    "\n",
    "Training and Fine-Tuning: Language models, such as pre-trained transformers like BERT or GPT, are often initially trained on large-scale, general-language datasets. However, to make them useful for specific NLP tasks, they are fine-tuned on task-specific datasets. This fine-tuning adapts the model's weights to perform well on the particular task.\n",
    "\n",
    "Application to Downstream Tasks: Once the model is fine-tuned, it is applied to the selected downstream tasks to make predictions or generate outputs. The model's performance on these tasks is evaluated using task-specific evaluation metrics.\n",
    "\n",
    "Extrinsic Measure Calculation: Extrinsic evaluation metrics are task-specific and can vary depending on the nature of the task. For example, accuracy, F1-score, BLEU score, ROUGE score, or Mean Average Precision (MAP) are common metrics used for various NLP tasks. These metrics quantify the model's ability to solve the task effectively.\n",
    "\n",
    "Analysis and Comparison: Researchers and practitioners analyze the extrinsic evaluation results to assess the model's performance on the downstream tasks. They may also compare the performance of different models or variations of the same model on the same tasks to determine which one performs better.\n",
    "\n",
    "Iterative Improvement: Based on the results, researchers may fine-tune the model further or experiment with different architectures, hyperparameters, or pre-processing techniques to improve the model's performance on specific tasks.\n",
    "\n",
    "The key idea behind extrinsic measures is to assess how well a language model or NLP system can contribute to real-world applications. By evaluating models in this manner, practitioners can gain insights into their practical utility and identify areas where improvements are needed. Extrinsic measures provide a more meaningful evaluation of language models compared to intrinsic measures (e.g., perplexity or word embeddings similarity) because they are tied to concrete, task-specific goals and applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?\n",
    "\n",
    "\n",
    "\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "In the context of machine learning, intrinsic measures and extrinsic measures are two different approaches to evaluating the performance of models or algorithms. They assess model quality and effectiveness in distinct ways. Here's an explanation of both:\n",
    "\n",
    "Intrinsic Measure:\n",
    "\n",
    "Definition: Intrinsic measures are evaluation metrics that assess the performance of a model or algorithm based on its internal characteristics or properties, often without direct reference to specific downstream tasks or applications.\n",
    "\n",
    "Usage: Intrinsic measures are typically used to evaluate models in a more general or abstract sense. They aim to provide insights into how well a model has learned from the data, its capacity to generalize, or its efficiency in terms of computational resources.\n",
    "\n",
    "Examples: Common intrinsic measures include metrics like accuracy, precision, recall, F1-score, perplexity (in the context of language models), and various statistical measures that quantify the quality of model outputs without considering their utility in real-world tasks.\n",
    "\n",
    "Purpose: Intrinsic measures are useful for benchmarking models, understanding their behavior, and identifying potential issues or limitations. They can be used during model development and optimization to guide decisions related to architecture, hyperparameters, and training strategies.\n",
    "\n",
    "Extrinsic Measure:\n",
    "\n",
    "Definition: Extrinsic measures, on the other hand, are evaluation metrics that assess the performance of a model or algorithm based on its effectiveness in solving specific real-world tasks or applications.\n",
    "\n",
    "Usage: Extrinsic measures focus on the practical utility of a model in real-world scenarios. They assess how well the model performs in the context of specific tasks or applications, such as image classification, language translation, speech recognition, or recommendation systems.\n",
    "\n",
    "Examples: Common extrinsic measures include accuracy, BLEU score (for machine translation), ROUGE score (for text summarization), Mean Average Precision (MAP, for information retrieval), and task-specific metrics used in areas like natural language processing, computer vision, and speech processing.\n",
    "\n",
    "Purpose: Extrinsic measures are used to evaluate whether a model is suitable for its intended application and how effectively it solves the targeted tasks. They are crucial for assessing a model's practical value and its impact on real-world problems.\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "The main differences between intrinsic and extrinsic measures are:\n",
    "\n",
    "Focus: Intrinsic measures assess the internal properties of a model or algorithm, while extrinsic measures assess its performance in specific real-world tasks.\n",
    "\n",
    "Purpose: Intrinsic measures are more suited for model development, fine-tuning, and understanding model behavior, while extrinsic measures are used to determine the practical utility of a model and its suitability for particular applications.\n",
    "\n",
    "Examples: Intrinsic measures include more general evaluation metrics like accuracy and F1-score, whereas extrinsic measures are often task-specific and tailor-made for particular applications.\n",
    "\n",
    "Usage: Intrinsic measures are commonly employed during model development and research, while extrinsic measures are used to evaluate models in production or deployment scenarios.\n",
    "\n",
    "In summary, intrinsic measures provide insights into a model's internal characteristics, whereas extrinsic measures assess a model's performance in real-world applications. Both types of measures are important in machine learning, as they serve different purposes in the evaluation and development of models and algorithms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?\n",
    "\n",
    "\n",
    "\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A confusion matrix is a fundamental tool in machine learning used to evaluate the performance of a classification model, especially in binary and multi-class classification tasks. Its primary purpose is to provide a detailed breakdown of the model's predictions and compare them to the actual class labels in order to assess the model's strengths and weaknesses.\n",
    "\n",
    "Here's how a confusion matrix serves its purpose and can be used to identify the strengths and weaknesses of a model:\n",
    "\n",
    "Purpose of a Confusion Matrix:\n",
    "\n",
    "Quantify Model Performance: It quantifies how well a classification model is performing by comparing its predictions to the actual ground truth.\n",
    "\n",
    "Evaluation Metrics Calculation: It serves as the basis for calculating various evaluation metrics that provide insights into the model's performance, including accuracy, precision, recall, F1-score, specificity, and false positive rate, among others.\n",
    "\n",
    "Identify Types of Errors: It helps identify the types of errors the model is making, such as false positives (Type I errors) and false negatives (Type II errors), which can be critical for understanding where the model may be failing.\n",
    "\n",
    "Using a Confusion Matrix to Identify Strengths and Weaknesses:\n",
    "\n",
    "Accuracy Assessment: The overall accuracy can be calculated from the confusion matrix to determine how often the model's predictions are correct. However, accuracy alone may not provide a complete picture.\n",
    "\n",
    "Precision and Recall Analysis: By examining the entries in the confusion matrix, you can calculate precision (the ability to make correct positive predictions) and recall (the ability to correctly identify all positives) for each class. This allows you to identify which classes the model is good at predicting and which it struggles with.\n",
    "\n",
    "F1-Score Analysis: The F1-score, which is the harmonic mean of precision and recall, provides a balanced measure that can help identify whether the model is performing well in terms of both false positives and false negatives.\n",
    "\n",
    "Class-Specific Analysis: The confusion matrix allows you to analyze the performance of the model for each class separately. This is important because a model may perform well for some classes but poorly for others.\n",
    "\n",
    "Error Diagnosis: You can use the confusion matrix to diagnose specific types of errors. For example, if a medical diagnostic model has a high false negative rate for a specific disease, it may indicate a critical weakness that needs attention.\n",
    "\n",
    "Threshold Tuning: In some cases, adjusting the decision threshold (the threshold for classifying an instance as positive or negative) can be an effective way to address certain weaknesses identified in the confusion matrix. For example, you can increase the threshold to reduce false positives at the cost of potentially increasing false negatives or vice versa.\n",
    "\n",
    "Model Improvement: Based on the strengths and weaknesses identified through the confusion matrix analysis, you can make informed decisions about model improvement, such as fine-tuning hyperparameters, collecting more data for underrepresented classes, or changing the model architecture.\n",
    "\n",
    "In summary, a confusion matrix is a critical tool in machine learning for assessing a classification model's performance and gaining insights into its strengths and weaknesses. It provides a detailed breakdown of predictions and actual class labels, allowing practitioners to fine-tune models and make informed decisions for improving classification accuracy and addressing specific challenges in different classes or categories.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?\n",
    "\n",
    "\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Evaluating the performance of unsupervised learning algorithms can be challenging because they don't rely on labeled data with predefined targets, as in supervised learning. Instead, intrinsic measures are used to assess the quality of the clustering or dimensionality reduction results produced by these algorithms. Here are some common intrinsic measures and how they can be interpreted:\n",
    "\n",
    "Silhouette Score:\n",
    "\n",
    "Interpretation: The silhouette score measures the quality of clusters in clustering algorithms. It quantifies how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The score ranges from -1 to 1, where a higher score indicates better-defined clusters.\n",
    "Interpretation:\n",
    "A score close to 1 suggests that data points are well clustered, with clear separation between clusters.\n",
    "A score close to 0 indicates overlapping clusters or clusters with data points that are not clearly assigned.\n",
    "A score close to -1 suggests that data points have been assigned to the wrong clusters.\n",
    "Davies-Bouldin Index:\n",
    "\n",
    "Interpretation: The Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster (separation) while also considering the average within-cluster similarity (compactness). Lower values indicate better clustering solutions, as it suggests well-separated and compact clusters.\n",
    "Interpretation:\n",
    "A lower Davies-Bouldin Index indicates better-defined and well-separated clusters.\n",
    "A higher index suggests that clusters are either too spread out or too similar to each other.\n",
    "Dunn Index:\n",
    "\n",
    "Interpretation: The Dunn Index measures the ratio between the minimum inter-cluster distance and the maximum intra-cluster distance. A higher Dunn Index indicates better clustering, as it reflects smaller intra-cluster variation and larger inter-cluster separation.\n",
    "Interpretation:\n",
    "A higher Dunn Index suggests more distinct and well-separated clusters.\n",
    "A lower index indicates that clusters are closer together or overlapping.\n",
    "Inertia (Within-Cluster Sum of Squares):\n",
    "\n",
    "Interpretation: Inertia measures the total distance between data points and their cluster centroids. It quantifies the compactness of clusters, with lower values indicating more compact clusters.\n",
    "Interpretation:\n",
    "Lower inertia values suggest tighter clusters with data points closer to their centroids.\n",
    "Higher inertia values may indicate that clusters are more spread out.\n",
    "Explained Variance (PCA):\n",
    "\n",
    "Interpretation: For dimensionality reduction using Principal Component Analysis (PCA), explained variance measures the proportion of total variance in the data explained by each retained principal component. It helps determine how much information is retained with a specific number of components.\n",
    "Interpretation:\n",
    "Higher explained variance for a given number of components indicates that those components capture more of the original data's variability.\n",
    "Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI)**:\n",
    "\n",
    "Interpretation: These measures assess the similarity between the ground truth labels and the cluster assignments produced by the algorithm. They provide insight into how well the algorithm has recovered the true underlying structure in the data.\n",
    "Interpretation:\n",
    "Higher ARI and NMI values suggest a better match between the true labels and the cluster assignments.\n",
    "In summary, intrinsic measures are essential for evaluating the performance of unsupervised learning algorithms. They provide insights into the quality of clustering solutions or dimensionality reduction results based on inherent characteristics of the data. Interpretation of these measures should consider the specific algorithm used, the nature of the data, and the problem's goals, as different measures may be more suitable for different scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?\n",
    "\n",
    "\n",
    "\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While accuracy is a commonly used metric for evaluating classification models, it has some limitations when used as the sole evaluation metric. These limitations arise from the fact that accuracy does not provide a complete picture of a model's performance, especially in situations where the data is imbalanced or when certain types of errors are more critical than others. Here are some limitations of accuracy and how they can be addressed:\n",
    "\n",
    "1. Sensitivity to Class Imbalance:\n",
    "\n",
    "Limitation: Accuracy can be misleading when dealing with imbalanced datasets, where one class significantly outnumbers the others. In such cases, a model that predicts the majority class for all instances can still achieve high accuracy but provide no value.\n",
    "Addressing: Consider using alternative metrics like precision, recall, F1-score, or area under the Receiver Operating Characteristic (ROC-AUC) curve that provide a more balanced assessment of model performance, especially for minority classes.\n",
    "2. Different Costs of Errors:\n",
    "\n",
    "Limitation: In some scenarios, false positives (Type I errors) and false negatives (Type II errors) have different consequences or costs. Accuracy treats all errors equally.\n",
    "Addressing: Define and use a metric that aligns with the specific costs or consequences of errors. For example, precision may be more relevant when minimizing false positives is crucial, while recall may be more relevant when minimizing false negatives is a priority. You can also use cost-sensitive learning techniques or custom loss functions to address this issue.\n",
    "3. Ambiguity in Probabilistic Predictions:\n",
    "\n",
    "Limitation: Accuracy does not consider the uncertainty or confidence in model predictions. It treats all predictions as equally certain, even when the model is uncertain about some instances.\n",
    "Addressing: Utilize probabilistic classification models (e.g., logistic regression, Bayesian classifiers) that provide probability estimates for each class. You can then use probabilistic evaluation metrics such as log-loss, Brier score, or calibration curves to assess prediction uncertainty and confidence.\n",
    "4. Class Distribution Shifts:\n",
    "\n",
    "Limitation: Accuracy may not capture changes in the class distribution over time or across different datasets. A model trained on one dataset may perform poorly when applied to a different dataset with a different class distribution.\n",
    "Addressing: Monitor model performance over time, especially in applications with evolving data, and consider retraining the model when the class distribution changes significantly. Additionally, use evaluation metrics that are less sensitive to distribution shifts, such as Kappa statistic or Matthews Correlation Coefficient (MCC).\n",
    "5. Misleading in Multiclass Problems:\n",
    "\n",
    "Limitation: In multiclass classification, accuracy may not adequately reflect the model's ability to distinguish between different classes, as it treats all classes equally.\n",
    "Addressing: Use metrics like macro-average or micro-average F1-score, confusion matrices, or class-specific evaluation metrics to gain insights into the model's performance for each class individually. These metrics provide a more granular view of class-specific strengths and weaknesses.\n",
    "In summary, while accuracy is a useful metric, it should not be the sole criterion for evaluating classification models, especially in situations with class imbalance, varying costs of errors, or different performance requirements. It is essential to select evaluation metrics that align with the specific goals and challenges of the classification task to obtain a more comprehensive understanding of model performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea60e2-957e-4403-9dab-7b8211371fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
