{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6dbbe7-509d-424a-8bd6-3dfebd31c97f",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd0b4dcf-bb16-4f12-a2e4-c7f42a02625d",
   "metadata": {},
   "source": [
    "ans \n",
    "Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large and val/test error is large too and high bias and high variance.\n",
    "\n",
    "Overfitting means that your model makes not accurate predictions. In this case, train error is very small and val/test error is large and have low bias and high variance.\n",
    "\n",
    "generalize model: train and test have high accuracy *(low bias ans low variance)\n",
    "\n",
    "Experts suggest that this problem can be alleviated by simply using more (good!) data for the project. In addition, the following ways can also be used to tackle underfitting.\n",
    "\n",
    "Increase the size or number of parameters in the ML model.\n",
    "Increase the complexity or type of the model.\n",
    "Increasing the training time until cost function in ML is minimised.\n",
    "Example: Converting a linear model’s data into non-linear data. In this case, the transformation of the model leads to it being more unpredictable with respect to any new as well as training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8feac46-1753-40a9-9434-a683d64760d9",
   "metadata": {},
   "source": [
    "Ques2 How can we reduce overfitting ?Explain in brief."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7491fba8-0c9e-4b1b-b1cd-845f06fc2b76",
   "metadata": {},
   "source": [
    "ans\n",
    "Overfitting occurs when the model performs well on training data but generalizes poorly to unseen data. Overfitting is a very common problem in Machine Learning and there has been an extensive range of literature dedicated to studying methods for preventing overfitting.\n",
    "1. Hold-out\n",
    "2. Cross-validation\n",
    "3. Data augmentation\n",
    "4. Feature selection\n",
    "5. L1 / L2 regularization\n",
    "6. Remove layers / number of units per layer\n",
    "7. Dropout\n",
    "8. Early stopping\n",
    "\n",
    "1. Hold-out (data)\n",
    "Rather than using all of our data for training, we can simply split our dataset into two sets: training and testing. A common split ratio is 80% for training and 20% for testing. We train our model until it performs well not only on the training set but also for the testing set. This indicates good generalization capability since the testing set represents unseen data that were not used for training. However, this approach would require a sufficiently large dataset to train on even after splitting.\n",
    "\n",
    "2. Cross-validation (data)\n",
    "We can split our dataset into k groups (k-fold cross-validation). We let one of the groups to be the testing set (please see hold-out explanation) and the others as the training set, and repeat this process until each individual group has been used as the testing set (e.g., k repeats). Unlike hold-out, cross-validation allows all data to be eventually used for training but is also more computationally expensive than hold-out.\n",
    "\n",
    "\n",
    "3. Data augmentation (data)\n",
    "A larger dataset would reduce overfitting. If we cannot gather more data and are constrained to the data we have in our current dataset, we can apply data augmentation to artificially increase the size of our dataset. For example, if we are training for an image classification task, we can perform various image transformations to our image dataset (e.g., flipping, rotating, rescaling, shifting).\n",
    "\n",
    "\n",
    "4. Feature selection (data)\n",
    "If we have only a limited amount of training samples, each with a large number of features, we should only select the most important features for training so that our model doesn’t need to learn for so many features and eventually overfit. We can simply test out different features, train individual models for these features, and evaluate generalization capabilities, or use one of the various widely used feature selection methods.\n",
    "\n",
    "\n",
    "5. L1 / L2 regularization (learning algorithm)\n",
    "Regularization is a technique to constrain our network from learning a model that is too complex, which may therefore overfit. In L1 or L2 regularization, we can add a penalty term on the cost function to push the estimated coefficients towards zero (and not take more extreme values). L2 regularization allows weights to decay towards zero but not to zero, while L1 regularization allows weights to decay to zero.\n",
    "\n",
    "\n",
    "6. Remove layers / number of units per layer (model)\n",
    "As mentioned in L1 or L2 regularization, an over-complex model may more likely overfit. Therefore, we can directly reduce the model’s complexity by removing layers and reduce the size of our model. We may further reduce complexity by decreasing the number of neurons in the fully-connected layers. We should have a model with a complexity that sufficiently balances between underfitting and overfitting for our task.\n",
    "\n",
    "\n",
    "7. Dropout (model)\n",
    "By applying dropout, which is a form of regularization, to our layers, we ignore a subset of units of our network with a set probability. Using dropout, we can reduce interdependent learning among units, which may have led to overfitting. However, with dropout, we would need more epochs for our model to converge.\n",
    "\n",
    "\n",
    "8. Early stopping (model)\n",
    "We can first train our model for an arbitrarily large number of epochs and plot the validation loss graph (e.g., using hold-out). Once the validation loss begins to degrade (e.g., stops decreasing but rather begins increasing), we stop the training and save the current model. We can implement this either by monitoring the loss graph or set an early stopping trigger. The saved model would be the optimal model for generalization among different training epoch values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1046ae3-642c-4fbe-911f-f6386aceecdb",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "raw",
   "id": "15775ae0-899d-426b-9a33-2e581335e4b8",
   "metadata": {},
   "source": [
    "ans\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset.\n",
    "A model under fits when it is too simple with regards to the data it is trying to model.\n",
    "\n",
    "One way to detect such a situation is to use the bias-variance approach, which can be represented like this:\n",
    "    Your model is under fitted when you have a high bias and high variance.\n",
    "    How to avoid underfitting :\n",
    "More data will not generally help. It will, in fact, likely increase the training error. Therefore we should increase more features. Because that expands the hypothesis space. This includes making new features from existing features. Same way more parameters may also expand the hypothesis space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7229a7-1245-49ca-b62d-549de9aca23c",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "297bfb01-e1be-4e32-9e3f-08575ca86682",
   "metadata": {},
   "source": [
    "ans\n",
    "\n",
    "It is important to understand prediction errors (bias and variance) when it comes to accuracy in any machine learning algorithm. There is a tradeoff between a model’s ability to minimize bias and variance which is referred to as the best solution for selecting a value of Regularization constant. Proper understanding of these errors would help to avoid the overfitting and underfitting of a data set while training the algorithm.\n",
    "\n",
    "Bias\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
    "By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as Underfitting of Data. This happens when the hypothesis is too simple or linear in nature. Refer to the graph given below for an example of such a situation.\n",
    "\n",
    "HighBias\n",
    "HighBias\n",
    "\n",
    "In such a problem, a hypothesis looks like follows.\n",
    "\n",
    "Variance\n",
    "The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high.\n",
    "While training a data model variance should be kept low.\n",
    "\n",
    "The high variance data looks like follows\n",
    "\n",
    "Bias Variance Tradeoff\n",
    "\n",
    "If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time. For the graph, the perfect tradeoff will be like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a9e2e-0bc7-45e4-8e45-654475f04a95",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51ea1457-b45e-432e-a4ed-5ca12d389e42",
   "metadata": {},
   "source": [
    "ans\n",
    "Overfitting and underfitting are common problems that can occur in machine learning models. Overfitting occurs when a model performs very well on the training data but poorly on the test data. Underfitting, on the other hand, occurs when a model is too simple and fails to capture the underlying patterns in the data, resulting in poor performance on both the training and test data. Here are some common methods to detect overfitting and underfitting:\n",
    "   1 Training and Test Error Curves: By plotting the training error and test error as a function of the model complexity, we can visually inspect whether the model is underfitting or overfitting. If the training and test errors are both high, the model is underfitting. If the training error is low but the test error is high, the model is overfitting.\n",
    "\n",
    "2 Cross-validation: Cross-validation is a technique for estimating the performance of a model by splitting the data into multiple training and test sets. By comparing the training and test error across the different folds, we can detect whether the model is overfitting or underfitting.\n",
    "\n",
    "3 Regularization: Regularization is a technique for preventing overfitting by adding a penalty term to the model's objective function. The penalty term encourages the model to have simpler parameters, which can help to prevent overfitting.\n",
    "\n",
    "4 Early stopping: Early stopping is a technique for preventing overfitting by stopping the training process before the model starts to overfit. By monitoring the test error during training, we can stop the training when the test error stops improving.\n",
    "\n",
    "5 Model complexity: Model complexity refers to the number of parameters or features in the model. If the model is too complex, it is more likely to overfit the data. If the model is too simple, it is more likely to underfit the data.\n",
    "\n",
    "To determine whether your model is overfitting or underfitting, you can use a combination of these methods. For example, you can plot the training and test error curves and use cross-validation to estimate the model's performance. If the training error is low but the test error is high, the model is overfitting. If both the training and test errors are high, the model is underfitting. You can then adjust the model's complexity or use regularization or early stopping to improve the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b5f83-d287-414f-a122-f4dea2719b1a",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e78bfd5-7eb2-4b97-a491-b1c09e20ae15",
   "metadata": {},
   "source": [
    "ans \n",
    "Bias and variance are two key concepts in machine learninig that are related to the ability of a model to capture the underlying pattern in the data.\n",
    "Bias refers to the  degree to which a model is able to capture the true \n",
    "relationship between the input features and output variable. A model with high bias is too simplistic and is likely to underfit the data.\n",
    " \n",
    "    variance ,on the other hand, refers to the degree to which a model is able to capture the noise or randomness in the data. A model with high variance is\n",
    "    too complex and is likely to overfit the data.\n",
    "    \n",
    "    High bias models are typically very simple and have limited capacity to capture the complexity of the data.Examples of high bias models include linear \n",
    "    regression model,which assume a linear relationship between the input features and the output variables,and decision trees with very few nodes.\n",
    "    High bias models tend to perform poorly on both the training  and test data because they are not able to capture the underlying patterns in the data.\n",
    "    \n",
    "    High variance models,on the other hand, are typically very complex and have high capacity to capture the complexity of the data.Examples of high variance models include deep neural networks with many  layers and features, and decision trees\n",
    "    with many nodes.High variance models tend to perform very well on the training data,but poorly on the test data because they are too complex and are overfitting the noise in the data.\n",
    "    \n",
    "    The performance of high bias and high variance models differs in terms of their ability to generlizs to \n",
    "    new data.High bias models have limited capacity to capture the complexity of the data, and\n",
    "    therefore tend to underfit the data.This results in poor performance on both the training and test data.\n",
    "    High variance models, on the other hand,have too much capacity and tend to overfit the data,resulting in excellent performance on the training data but poor performance on the test data.\n",
    "    To achieve optimal performance,it is important to strike a balance between bias and variance. This can be done by choosing a model with appropriate complexity ,using regularization techniques to \n",
    "    prevent overfittinig ,and using techniques such as cross-validation to estimate the model's \n",
    "    performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928c9c4-0932-417a-a3be-ba18da3ebd24",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "raw",
   "id": "991de8ad-26f4-4771-ba30-4e62715ace44",
   "metadata": {},
   "source": [
    "Ans\n",
    " \n",
    "    Regularization in machine learning is a set of techniques used to prevent overfitting,which occurs when a model fits the training data too closely and\n",
    "    become less effective at generalizing to new data.Regularization methods introduce additional constrints or penalitied to the model's\n",
    "    optimization process, encouraging it to learn simpler and generalizable patterns.\n",
    "    \n",
    "    There are several common regularization techniques used in machine learning:\n",
    "        1. L1 Regularization (Lasso Regression ):This method adds a penalty term to the\n",
    "        loss function that is proportional to the absolute value of the model's coefficients.\n",
    "        This encourages the model to eliminate unimportant features by setting their \n",
    "        corresponding coefficients to zero.\n",
    "        2. L2 Regularization(Ridge Regression): This method adds a penalty term to the \n",
    "        loss function that is proportional to the squared magnitude of the model's coefficients.\n",
    "        This encourages the model to learn smaller and smoother coefficients, which can help prevent overfiitting.\n",
    "        3.Dropout: This technique randomly drops our(sets to zero) some of the neurons in a neural network during training. This\n",
    "        helps prevent the network from relying too heavily on any single neuroon or feature ,and encourages it to learn more robust representations.\n",
    "         4. Early stopping: This technique stops the training process before the model has fully converged,\n",
    "            based on a performance metric on a validation set.This can help prevent overfitting by stopping the model\n",
    "            before it starts to fit noise in the training data.\n",
    "        5.Data augmentation: This technique involves generating new training data by applying transformations to the exisiting data, such as rotating or flipping\n",
    "        images. This can help prevent overfitting by increasing the diversity and quantity of training data.\n",
    "        \n",
    "        Overall, regularization techniques help to balance the trade-off between model complexitiy and generalization performance,\n",
    "        and are an important tool for building effective machine learning.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d19140-c26a-4ff2-8127-6e492c976013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
