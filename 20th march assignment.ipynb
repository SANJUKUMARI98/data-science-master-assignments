{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20th april assignment(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbors algorithm,also known as KNN or k-NN ,is \n",
    "non-parametric ,supervised learning classifier,which uses proximity\n",
    "to make classsification or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KNN (k-nearest neighbors),K represents the number of neighbors to consider when making a prediction.\n",
    "The value of k is a hyperparameter that needs to be chosen before training the model.The choice of  the value of K in KNN can significantly affect the model's performance.\n",
    "\n",
    "common method for selectinig the optimal value of k:\n",
    "\n",
    "- cross validation: In this ,the dataset is divided into multiple folds,and the model is trained and evaluate multiple time with different values of k. The average performance across of k that gives the best average performance is chosen as the optimal value of k.\n",
    "\n",
    "- odd value of k  in KNN: choosing an odd value of k helps to avoid ties when making predictions.for binary classification problems,it is generally recommended to choose an odd value of k to avoid ties in the majority vote.\n",
    "- Domain knowledge: if the problem at hand involves a large no of classes,a smaller value of k may be more appropriate.\n",
    "on the other ,if the dataset is noisy or contains outliers,a larger value of k may be more suitable to reduce the impact of noise.\n",
    "\n",
    "- Experimentation: Another approach is to simply experiment with different values of k and observe the performance of the model and select the best k value .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN classifier: it is used for classification tasks ,where the goal is to assign an input data point to one of several predefined classes of categories. In KNN classifiction,the output is a discrete class label.\n",
    "the KNN classifier determines the class label of an input data point by finding the k nearest neighbors in the training data and taking a majority vote among the k neighbors to determine the predicted \n",
    "class label.the predicted class label is then assigned to the input data point.\n",
    "\n",
    "\n",
    "KNN Regressor : It is used for regression tasks,where the goal is to predict a continous numerical value for an input data point.In KNN regression ,the output is a contionous numerical value.instead of taking a majority vote among ,it computes the average or weighted average of the target of the target values of the k nearest neighbors in the training dagta to determine  the predicted numerical value.\n",
    "the predicted numerical value is then assigned to the input data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of KNN can be measured by:\n",
    "\n",
    "- classification problem:\n",
    "\n",
    "Accuracy : it is the most commonly used metric for classifiction tasks.It measures the proportion of correctly predicted class labels to the total number of samples.Higher accuracy indicates better performance,but it may not be appropriate in cases where the classes are imbalanced.\n",
    "\n",
    "precision,recall,F1-score:These metrics provide insights into the model's performance in terms of true positive rate(TPR),false \n",
    "positive rate(FPR),and the trade- off between precision and recall.\n",
    "\n",
    "Precision is the ratio of true positives to the sum of true positive and false positives,\n",
    "\n",
    "recall: it is ratio of true positive to the sum of true positives and false negatives,F1-score is the harmonic mean of precision  and recall.\n",
    "\n",
    "confusion matrix : A confusion matrix is a table that shows the counts of true positive,true negative ,false positive,and false negative predictions made by the model.It provides a more detailed view of the model's performance and can be used to calculate other classification metrics.\n",
    "\n",
    "\n",
    "- Regression problem metrics:\n",
    "\n",
    "Mean squared error(MSE) : It is metric used for regression tasks.It measures the average squared difference  between the predicted and actual numerical values.Lower MSE indicates better performance.\n",
    "\n",
    "Root mean squared error(RMSE) : It is the square of root of MSE and provides the measure of the average absolute error between the predicted and actual values. lower RMSE  value indicates better performance.\n",
    "\n",
    "R-squared(R2) : it is a statistical measure the indicates teh proportion of the variance in the target variable that is explained by the model.R2 value close to 1 indicates better performance.\n",
    "\n",
    "Cross-validation : it is a common technique used to estimate the model's performance by splitting the data into multiple folds and training and evaluating the model on different folds.Cross-validation helps to obtain a more reliable estimate of the model's performance by reducing the variance inthe performance metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a phenomenon that occurs in machine learining ,including in the k-nearest neighbors algorithm,when the performance of a model deteriorates as the number of features or dimensions in the input data increases. The curse of dimensionality can adversely affect the performance of KNN in several ways:\n",
    "\n",
    "- Increased computation time : as the number of dimension increases,the distance calculation required for finding the k nearest neighbors become more computationally expensive,making it slower to find neighbors and make predictions\n",
    "\n",
    "- reduces effectiveness of distance-based similarity :KNN relies on calculating distance between data points to determine the nearest neighbors.In high-dimensional spaces,the distance between points tends to become less meaningful as the number of dimensions increases.KNN algorithm may struggle to accurately identify the true nearest neighbors,leading to poorer performance.\n",
    "\n",
    "- Increased risk of overfitting : In high-dimensional spaces,the available data becomes more sparse,meaning that the data points are farther apart from each other. this can increase the risk of overfitting in KNN,as the algorithm may rely on few neighbors that are not necessarily representative of the true underlying pattern in the data.This can lead to overfitting ,where the KNN model may not generalized well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values in K-nearest neighbors (KNN) can be done by either imputing missing values with a suitable value(like mean,median or mode) or using techniques such as k-NN  or distance -weighted imputation or distance- weighted imputation where missing values are estimated based on the values of their k-nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both have some similarities and differences in terms of performance and suitability for different type of problems:\n",
    "\n",
    "- similarities:\n",
    "Both KNN classifier and regressor are instance-based ,non-parametric algorithm algorithms that make predictions based on the proximity of training data points in the feature space.\n",
    "\n",
    "- Both KNN classifier and regressor are easy to implement and do not require complex model training or assumptions about the data distribution.\n",
    "\n",
    "Differences:\n",
    "\n",
    "- KNN classifier is used for classification tasks where the target variable is categorical,while in KNN regressor the target variable is continuous.\n",
    "- performance evaluation metrics classification problem use accuracy,precision,recall,F1-score,confusion-matrix while regression problem use MSE,RMSE,R-squared.\n",
    "- KNN classifier is more sensitive to the choice of hyperparameters,such as the number of neighborsK and the distance metric,as it can affect the decision boundary and classification results.\n",
    "KNN regressor is affected by the choice of k,but it may also be influenced by the scale of the target variable and distribution of data.\n",
    "- KNN classifier may perform well when dealing with  discrete ,categorical data,and imbalanced classes,while KNN regressor may perform well when dealing with continuous ,numerical  data with a smooth relationship between features and target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths of K-nearest neighbors(KNN)algorithm:\n",
    "\n",
    "1.Simple and easy to understand,with no assumptions about data distribution.\n",
    "2.can be used for both classification and regression tasks.\n",
    "3.Non-parametric nature makes it suitable for handling non-linear relationships in data.\n",
    "4.Can work well for small datasets or when there is no clear mathematical relationship between features and target variable.\n",
    "\n",
    "Weaknesses of KNN algorithm:\n",
    "\n",
    "1.Can be computationally expensive for large datasets,as distance calculations for finding nearest neighbors can be time-consuming .\n",
    "2.Sensitive to the choice of hyperparameters ,such as the number of neighbors(K) and distance metric.\n",
    "3 can be affected by noide and outliers in data.\n",
    "4.performance can degrade in high-dimensional feature spaces due to the curse of dimensionality.\n",
    "\n",
    "Addressing these weaknesses:\n",
    "\n",
    "1.using techniques such as local search or approximate nearest neighbor search to speed up computation.\n",
    "2.Tunining hyperparameters ,such as choosing an optimal value for K through cross-validation.\n",
    "3.Applyingi data preprocessing techniques,such as outlier detection and removal ,and data normalization.\n",
    "4.using feature selection or dimensionality reduction methods to reduce the number of dimensions in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both is used to calculate distance in k-nearest neighbor KNN algorithm.\n",
    "\n",
    "Euclidean distance:it is  the square root of the sum of squared differences in all dimensions.it considers both magnitudes and directionn.\n",
    "\n",
    "while manhattan distance is the sum of absolute differences in all dimensions.it only considers magnitude along each axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of feature scaling in KNN is important because:\n",
    "1. It ensures that all features or dimensions of the data are on a similar scale,thereby preventing any one feature from dominating the distance calculation.\n",
    "\n",
    "2.Since KNN relies on calculating distances between data points to determine similarity or proximity,features with larger scales may have a disproportionate influence on the distance calculation,leading to biased results.\n",
    "\n",
    "3.Feature scaling,such as normalization or standardization ,brings all features to a similar scale,allowiniig KNN to give equal importance to all features and produce more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
