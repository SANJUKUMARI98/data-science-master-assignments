{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "\n",
    "Gradient Boosting Regression is a popular machine learning technique used for solving regression problems. It is an ensemble learning method that combines multiple weak regression models (typically decision trees) to create a strong regression model. The key idea of Gradient Boosting Regression is to sequentially train weak regression models on the residuals (i.e., the differences between the predicted values and the true target values) of the previous models, in order to gradually reduce the errors and improve the overall prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.991148726134352\n"
     ]
    }
   ],
   "source": [
    "# make dataset for training\n",
    "from sklearn.datasets import make_regression\n",
    "x,y = make_regression(n_samples =1000,n_features=4,n_informative=2,\n",
    "                      random_state=0,shuffle=False)\n",
    "\n",
    "\n",
    "#train the model \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42)\n",
    "\n",
    "\n",
    "# load the algorithm to train the model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# train the model by gradient boosting classifier\n",
    "regressor = GradientBoostingRegressor()\n",
    "regressor.fit(x_train,y_train)\n",
    "\n",
    "#predict the result of the model\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "#load the metric to  check the accuracy of the model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('r2_score:', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.9910966466797334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9547444847128791"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataset for training\n",
    "from sklearn.datasets import make_regression\n",
    "x,y = make_regression(n_samples = 1000,n_features=4,n_informative=2,\n",
    "                      random_state=0,shuffle=False)\n",
    "\n",
    "\n",
    "#train the model \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42)\n",
    "\n",
    "\n",
    "# load the algorithm to train the model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# train the model by gradient boosting classifier\n",
    "regressor = GradientBoostingRegressor()\n",
    "regressor.fit(x_train,y_train)\n",
    "\n",
    "#predict the result of the model\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "#load the metric to  check the accuracy of the model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('r2_score:', r2_score(y_test,y_pred))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':(100,150,250,200,350),'learning_rate':[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0],'min_samples_split':[1,2,4,3,4,5,6,7]}\n",
    "\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "clf = GridSearchCV(regressor,param_grid=parameters,cv=5)\n",
    "\n",
    "# splitting of training data to train and validation\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "clf.best_params_\n",
    "\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1.0, 'min_samples_split': 4, 'n_estimators': 250}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple and relatively low-performing regression or classification model that is used as a building block in the ensemble. It typically has low complexity, such as a shallow decision tree with few nodes, and is combined with other weak learners to create a strong and more accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to sequentially improve the predictions of a weak learner by building an ensemble of weak learners.\n",
    "\n",
    " The algorithm iteratively fits a weak learner to the residuals (i.e., the differences between the predicted values and the true target values) of the previous model in the ensemble.\n",
    " \n",
    "  This allows the subsequent models to focus on correcting the errors made by the previous models, gradually reducing the overall prediction errors. The residuals are updated using a gradient descent optimization technique, which guides the algorithm to adjust the model parameters in the direction that minimizes the residual errors. \n",
    "  \n",
    "  This process continues until a predefined number of weak learners are combined to create a strong and accurate predictive model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. The process starts with fitting an initial weak learner to the input data. \n",
    "\n",
    "Then, the residuals (i.e., the differences between the predicted values and the true target values) of this initial model are calculated. The subsequent weak learners are then trained to predict these residuals. \n",
    "\n",
    "The predicted residuals are added to the previous predictions, and this process is repeated for a predefined number of iterations. The final ensemble is created by combining the predictions of all the weak learners, typically through weighted averaging.\n",
    "\n",
    " This sequential approach allows the weak learners to focus on correcting the errors made by the previous models, resulting in a strong and accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "Initialization: The process starts with initializing the predictions of the ensemble with a base model, typically a simple regression or classification model. This initial model is often a constant value, such as the mean of the target values for regression or the mode of the target values for classification.\n",
    "\n",
    "Calculating Residuals: The residuals, which are the differences between the predicted values and the true target values, are calculated for the initial model. These residuals represent the errors made by the initial model and serve as the target values for the subsequent weak learners to correct.\n",
    "\n",
    "Fitting Weak Learners: A weak learner, such as a decision tree with few nodes, is fit to the residuals of the previous model. This weak learner aims to capture the patterns in the residuals and make better predictions for the target values.\n",
    "\n",
    "Updating Predictions: The predictions of the ensemble are updated by adding the predictions of the weak learner, multiplied by a learning rate or step size, to the previous predictions. This update is done in the direction that minimizes the residual errors, using a gradient descent optimization technique.\n",
    "\n",
    "Repeating Steps 2-4: Steps 2-4 are repeated for a predefined number of iterations. In each iteration, the residuals are updated based on the errors of the previous model, and a new weak learner is fit to the updated residuals. The predictions of the ensemble are then updated accordingly.\n",
    "\n",
    "Ensemble Prediction: The final prediction of the ensemble is created by combining the predictions of all the weak learners, typically through weighted averaging. The weights may be determined based on the performance of each weak learner or assigned uniformly.\n",
    "\n",
    "Regularization: Regularization techniques, such as adding a penalty term for model complexity or limiting the number of iterations, can be applied to prevent overfitting and improve the generalization performance of the Gradient Boosting algorithm.\n",
    "\n",
    "These steps are repeated iteratively until a predefined stopping criterion is met, resulting in an ensemble of weak learners that collectively form a strong and accurate predictive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
